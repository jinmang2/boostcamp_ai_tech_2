# Byte Pair Encoding

ì„ íƒ ê³¼ì œ BPEë¥¼ ìˆ˜í–‰í•˜ê¸° ì „, ì•„ì˜ˆ ê³µë¶€ ë‹¤ í•´ë²„ë¦¬ê³  ì •ë¦¬í•œ ë¬¸ì„œ

ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²• ë° ë™ì‘ê³¼ì • ìƒì„¸ë„ ì •ë¦¬
- VKCOM, `YouTokenToMe`
- huggingface, `Tokenizers`

## What is BPE?
- ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•´ í•˜ë‚˜ì˜ Embeddingì„ ìƒì„±í•  ê²½ìš°, Out-of-vocabulary(OOV)ë¼ëŠ” ì¹˜ëª…ì ì¸ ë¬¸ì œê°€ ìƒê¸´ë‹¤.
- ì¶”ë¡  ì‹œ í•™ìŠµ ë•Œ ì—†ë˜ ë‹¨ì–´ê°€ ë“±ì¥í•˜ë©´ í•´ë‹¹ ìœ„ì¹˜ì— `[UNK]` í† í°ì´ ë“¤ì–´ê°€ì„œ ì „ì²´ì ì¸ ëª¨ë¸ ì„±ëŠ¥ì´ í•˜ë½í•  ìˆ˜ ìˆë‹¤.
    - uncasedë¥¼ ë§ì´ ë§Œë“¤ì–´ì„œ ëŒ€ì‘í•  ìˆ˜ ìˆì§€ë§Œ, ë¶€ì¡±í•˜ë‹¤
- ë°˜ë©´ ëª¨ë“  ë‹¨ì–´ì˜ embeddingì„ ë§Œë“¤ê¸°ì—ëŠ” í•„ìš”í•œ embedding parameterì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ë‹¤.
    - ZeRO-offloadì—ì„œë„ embedding matrixëŠ” ë‚´ë¦¬ê¸°ê°€ ì•ˆëœë‹¤ê³  í•œë‹¤ (í˜„ì›…ì´ í”¼ì…œ)
- ìœ„ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì»´í“¨í„°ê°€ ì´í•´í•˜ëŠ” ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ëŠ” ë°ì— ë°ì´í„° ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ `Byte Pair Encoding` ê¸°ë²•ì„ ì ìš©í•œ sub-word tokenizationì´ë¼ëŠ” ê°œë…ì´ ë“±ì¥

## BPE ì†Œê°œ ë…¼ë¬¸ (subword unitì— ì ìš©ëœ ë²„ì „)

[Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/)

í•„ìš”í•œ ë¶€ë¶„ë§Œ ë°œì·Œí•´ì„œ ì½ê¸°

- Main Contribution
    1. Open Vocabulary NMTëŠ” subword unitìœ¼ë¡œ encodingí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ëŠ¥í•¨
        - large vocab í˜¹ì€ back-off dictionariesë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ íš¨ê³¼ê°€ ì¢‹ìŒ
    2. Byte Pair Encodingì„ word segmentation task ì•Œê³ ë¦¬ì¦˜ì— ì ìš©
        - NMTì— ì•„ì£¼ ì ì ˆí•œ word segmentation ì „ëµ
        - BPE, Philip Gage.  1994.  A New Algorithm for Data Com-pression.C Users J., 12(2):23â€“38, February.
- Hypothesis: a segmentation of rare words into appropriate subword unitsëŠ” ì•„ë˜ë¥¼ í•˜ê¸° ì¶©ë¶„í•¨
    - to allow for the NMT to learn transparent translations
    - to generalize this knowledge to translate and produce unseen words
- BPE ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…
    - Gageê°€ 1994ë…„ì— ë°œí‘œí•œ ê°„ë‹¨í•œ ë°ì´í„° ì••ì¶• ì „ëµ
        - iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte
    - ë³¸ ì—°êµ¬ì—ì„  Word Segmentationì— BPEë¥¼ ì ìš©
        1. char-vocabìœ¼ë¡œ symbol vocabì„ ì´ˆê¸°í™”í•˜ê³  ë‹¨ì–´ë¥¼ sequence of charactersë¡œ í‘œí˜„
            - ì—¬ê¸°ì„œ special end-of-word symbol 'Â·'ë¥¼ ì¶”ê°€í•´ì¤€ë‹¤.
            - ì´ëŠ” ë³µì›ì„ ìœ„í•´ ë„£ì–´ì£¼ëŠ” special token
        2. ì•„ë˜ taskë¥¼ ë°˜ë³µí•œë‹¤
            a. symbol pairë“¤ì„ ì„¼ë‹¤
            b. ê°€ì¥ ë¹ˆë²ˆíˆ ë“±ì¥í•˜ëŠ” ('A', 'B')ë¥¼ ìƒˆë¡œìš´ symbol 'AB'ë¡œ ëŒ€ì²´í•œë‹¤
                - ìœ„ ê³¼ì •ì€ `merge operation`ì´ë¼ ë¶€ë¥¸ë‹¤
                - ì´ ì—°ì‚°ìœ¼ë¡œ char-n-gramì— í•´ë‹¹í•˜ëŠ” ìƒˆë¡œìš´ symbolì´ ìƒì‚°ëœë‹¤
                - ë¹ˆë²ˆí•œ char-n-gramì€ ê²°êµ­ ë‹¨ì¼ ê¸°í˜¸ë¡œ ë³‘í•©ë˜ë¯€ë¡œ BPEì—ëŠ” shortlist(í›„ë³´ ëª©ë¡)ì´ í•„ìš”ì—†ë‹¤
        3. ìµœì¢… symbol vocab sizeëŠ” ì´ˆê¸° vocab size + number of merge operationì™€ ë™ì¼í•˜ë‹¤
            - í›„ìëŠ” ìœ ì¼í•œ BPEì˜ hyperparameter

![img](../../../assets/img/u-stage/nlp_bpe_01.PNG)

- ë…¼ë¬¸ì—ì„œ í‰ê°€ì˜ ëª©ì 
    - NMTì—ì„œ subword unitìœ¼ë¡œ rare, unseend wordì— ëŒ€í•œ ë²ˆì—­ë¥ ì„ ì˜¬ë¦´ ìˆ˜ ìˆì„ê¹Œ? -> ê°€ëŠ¥
    - vocab size, text size, ë²ˆì—­ í’ˆì§ˆ ì¸¡ë©´ì„¸ì–´ ê°€ì¥ ì˜ ìˆ˜í–‰ë˜ëŠ” subword segmentationì€ ë¬´ì—‡? -> BPE

![img](../../../assets/img/u-stage/nlp_bpe_02.PNG)

- ì§„ì§œ ì••ë„ì ìœ¼ë¡œ ì ì€ UNK tokens... ã„·ã„·

## Optional Homework
- ìœ„ ì•Œê³ ë¦¬ì¦˜ 1ì„ ì°¸ê³ í•˜ì—¬ BPEë¥¼ êµ¬ì¶•í•˜ë¼!
- ê³¼ì œ ì½”ë“œë¥¼ ì—¬ê¸°ì— ì‘ì„±í•˜ê¸°ëŠ” ê·¸ë ‡ê¸° ë•Œë¬¸ì—, ì •í™•íˆ ì´í•´í•œ ë‹¤ìŒ ë‚˜ë§Œì˜ ì½”ë“œ ëŠë‚Œìœ¼ë¡œ ì½”ë“œ ì‘ì„±
- ê³¼ì œ ì½”ë“œ class, í•¨ìˆ˜ ì´ë¦„, test caseë¥¼ ìˆ˜ì •í•˜ì—¬ bpe.pyì—ë‹¤ ìˆ˜í–‰ ë‚´ì—­ ê¸°ë¡
- testcaseëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŒ

## ğŸ’¥ Tokenizers
- https://github.com/huggingface/tokenizers
- Fast SOTA Tokenizers optimized for Research and Production

### Main features
- Rust êµ¬í˜„ìœ¼ë¡œ ë§¤ìš° ë¹ ë¦„ (in train and tokenization)
- ì„œë²„ CPUì—ì„œ 1GBì˜ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ”ë° 20ì´ˆ ë¯¸ë§Œì´ ê±¸ë¦¼
- ì‚¬ìš©í•˜ê¸° ì‰½ì§€ë§Œ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ tokenizerìˆìŒ
- research and productionìš©
- Normalizationì€ alignment trackingê³¼ í•¨ê»˜ ì œê³µ
- ì£¼ì–´ì§„ tokenì— í•´ë‹¹í•˜ëŠ” ì›ë˜ ë¬¸ì¥ì˜ ì¼ë¶€ë¥¼ ì–»ëŠ” ê²ƒì´ ê°€ëŠ¥
- ì•„ë˜ì˜ ëª¨ë“  ì „ì²˜ë¦¬ê°€ ê°€ëŠ¥
    - Truncate
    - Pad
    - Add the special tokens your model needs

### Quick example using Python
BPE, WordPiece or Unigram ì¤‘ í•˜ë‚˜ë¥¼ ê³ ë¥¸ë‹¤
```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE())
```

pre-tokenizationì„ customize ê°€ëŠ¥ (e.g., splitting into word)
```python
from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()
```

ì•„ë˜ ë‘ ì¤„ë¡œ í•™ìŠµ ê°€ëŠ¥

```python
from tokenizers.trainers import BpeTrainer

train_data_path = ["train_data.txt"]
# train_data_path = ["wiki.train.raw", "wiki.valid.raw", "wiki.test.raw"]

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
tokenizer.train(files=train_data_path, trainer=trainer)

print(dict(sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])))
```
```
[00:00:00] Pre-processing files (0 Mo)              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                100%
[00:00:00] Tokenize words                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4        /        4
[00:00:00] Count pairs                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4        /        4
[00:00:00] Compute merges                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12       /       12

{'[UNK]': 0, '[CLS]': 1, '[SEP]': 2, '[PAD]': 3, '[MASK]': 4, 'd': 5, 'e': 6, 'i': 7,
 'l': 8, 'n': 9, 'o': 10, 'r': 11, 's': 12, 't': 13, 'w': 14, 'es': 15, 'est': 16,
 'lo': 17, 'low': 18, 'ew': 19, 'new': 20, 'newest': 21, 'dest': 22, 'idest': 23,
 'widest': 24, 'er': 25, 'lower': 26}
```

í—ˆê¹…í˜ì´ìŠ¤ í† í¬ë‚˜ì´ì €ì˜ ì¥ì ? hubì— ì˜¬ë¦´ ìˆ˜ ìˆë‹¤!
sentencepieceë„ ì˜¬ë¦¬ë˜ë° yttmë„ ì˜¬ë¦´ ìˆ˜ ìˆë‚˜?

## YouTokenToMe
- https://github.com/VKCOM/YouTokenToMe
- Unsupervised text tokenizer focused on a computational efficiency
- Fast BPEë¥¼ êµ¬í˜„
- [huggingfaceì˜ Tokenizer](https://github.com/huggingface/tokenizers), [fastBPE](https://github.com/glample/fastBPE), [Sentencepiece](https://github.com/google/sentencepiece)ë³´ë‹¤ ë¹ ë¥´ë‹¤ê³  í•¨

### Key advantages
- Multithreading for training and tokenization
- The algorithm has `O(N)` complexity, wher `N` is the length of training data
- Highly efficient implementation in C++
- Python wrapper and command-line interface

### Extra features
- BPT-dropout (as described in [Provikov et al., 2019](https://arxiv.org/abs/1910.13267))

BPE ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ cross word boundariesì— ëŒ€í•´ì„  ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. Sentencepieceì™€ ê°™ì´ ëª¨ë“  space symbolì€ meta symbol  "â–" (U+2581)ë¡œ ëŒ€ì²´ëœë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë“  sequence tokenì„ textë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ê³  word boundaryë¥¼ ë³µì›í•  ìˆ˜ ìˆë‹¤.

### Installation

```
pip install youtokentome
```

ìì„¸í•œ ì‚¬ìš© ë°©ë²•ì€ youtokentome readme ì°¸ê³ !

ì´ ë²ˆ ê³¼ì œë¥¼ `YouTokenToMe`ë¡œ í‘¼ë‹¤ë©´,

```python
import youtokentome as yttm

train_data_path = "train_data.txt"
model_path = "example.model"

corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3

with open(train_data_path, "w") as fout:
    for word in corpus:
        print(word, file=fout)

# Training model
yttm.BPE.train(data=train_data_path, vocab_size=5000, model=model_path)
```
```
Training parameters
  input: train_data.txt
  model: example.model
  vocab_size: 5000
  n_threads: 8
  character_coverage: 1
  pad: 0
  unk: 1
  bos: 2
  eos: 3

reading file...
learning bpe...
number of unique characters in the training data: 10
number of deleted characters: 0
number of unique characters left: 10
WARNING merged only: 30 pairs of tokens
model saved to: example.model
<youtokentome.youtokentome.BPE object at 0x7fef35f5c700>
```

```python
# Loading model
bpe = yttm.BPE(model=model_path)
bpe.vocab()
```
```
['<PAD>', '<UNK>', '<BOS>', '<EOS>', 'â–', 'e', 'w', 't', 's', 'o', 'l', 'n', 'i',
 'd', 'r', 'es', 'est', 'lo', 'low', 'â–low', 'west', 'â–n',
 'ewest', 'â–newest', 'dest', 'â–w', 'idest', 'â–widest', 'â–lowe', 'â–lower']
```
