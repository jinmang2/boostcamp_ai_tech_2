# 7ê°• BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë¬¸ì¥ í† í° ë¶„ë¥˜

<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#71-ë¬¸ì¥-í† í°-ë¶„ë¥˜-task-ì†Œê°œ">ë¬¸ì¥ í† í° ë¶„ë¥˜ task ì†Œê°œ</a>
      <ul>
        <li><a href="#11-task-ì†Œê°œ">task-ì†Œê°œ</a></li>
        <li><a href="#12-ë¬¸ì¥-token-ë¶„ë¥˜ë¥¼-ìœ„í•œ-ë°ì´í„°">ë¬¸ì¥ token ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°</a></li>
      </ul>
    </li>
    <li>
      <a href="#72-ë¬¸ì¥-í† í°-ë¶„ë¥˜-ëª¨ë¸-í•™ìŠµ">ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹¤ìŠµ</a>
      <ul>
        <li><a href="#21-ë¬¸ì¥-í† í°-ë¶„ë¥˜-ëª¨ë¸-í•™ìŠµ">ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ</a></li>
        <li><a href="#ë¬¸ì¥-í† í°-ë‹¨ìœ„-ë¶„ë¥˜-ëª¨ë¸-í•™ìŠµ">ë¬¸ì¥ í† í° ë‹¨ìœ„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ</a></li>
        <li><a href="#ê¸°ê³„-ë…í•´-ëª¨ë¸-í•™ìŠµ">ê¸°ê³„ ë…í•´ ëª¨ë¸ í•™ìŠµ</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ol>
</details>

## 1. ë¬¸ì¥ í† í° ë¶„ë¥˜ task ì†Œê°œ

### 1.1 task ì†Œê°œ

- ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° tokenì´ ì–´ë–¤ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” task

![img](../../../assets/img/p-stage/klue_07_01.PNG)

#### NER (Named Entity Recognition)
- ê°œì²´ëª… ì¸ì‹ì€ ë¬¸ë§¥ì„ íŒŒì•…í•´ì„œ ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“±ê³¼ ê°™ì€ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì„œì—ì„œ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì–´, ë˜ëŠ” ì–´êµ¬(ê°œì²´) ë“±ì„ ì¸ì‹í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤

![img](../../../assets/img/p-stage/klue_07_02.PNG)

- ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì—ì„œ ë‹¤ì–‘í•œ ê°œì²´(Entity)ë¡œ ì‚¬ìš©

![img](../../../assets/img/p-stage/klue_07_03.PNG)

#### POS (Part-of-speech tagging)
- í’ˆì‚¬ë€ ë‹¨ì–´ë¥¼ ë¬¸ë²•ì  ì„±ì§ˆì˜ ê³µí†µì„±ì— ë”°ë¼ ì–¸ì–´í•™ìë“¤ì´ ëª‡ ê°ˆë˜ë¡œ ë¬¶ì–´ ë†“ì€ ê²ƒ
- í’ˆì‚¬ íƒœê¹…ì€ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° ì„±ë¶„ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì•Œë§ëŠ” í’ˆì‚¬ë¥¼ íƒœê¹…í•˜ëŠ” ê²ƒì„ ì˜ë¯¸

![img](../../../assets/img/p-stage/klue_07_04.PNG)

```
from pororo import Pororo
pos = Pororo(task="pos", lang="ko")
pos("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.")
```

<br/>
<div align="right">
    <b><a href="#7ê°•-bert-ì–¸ì–´ëª¨ë¸-ê¸°ë°˜ì˜-ë¬¸ì¥-í† í°-ë¶„ë¥˜">â†¥ back to top</a></b>
</div>
<br/>

### 1.2 ë¬¸ì¥ token ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°

#### kor_ner
- í•œêµ­í•´ì–‘ëŒ€í•™êµ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì‹¤ì—ì„œ ê³µê°œí•œ í•œêµ­ì–´ NER ë°ì´í„°ì…‹
- ì¼ë°˜ì ìœ¼ë¡œ, NER ë°ì´í„°ì…‹ì€ pos taggingë„ í•¨ê»˜ ì¡´ì¬

![img](../../../assets/img/p-stage/klue_07_05.PNG)

- Entity tagì—ì„œ Bì˜ ì˜ë¯¸ëŠ” ê°œì²´ëª…ì˜ ì‹œì‘(Begin)ì„ ì˜ë¯¸í•˜ê³ , Iì˜ ì˜ë¯¸ëŠ” ë‚´ë¶€(Inside)ë¥¼ ì˜ë¯¸í•˜ë©°, OëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ” ê°œì²´ëª…(Object)ë¥¼ ì˜ë¯¸
- ì¦‰, B-PERì€ ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ì‹œì‘ì„ ì˜ë¯¸
- I-PERì€ ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ë‚´ë¶€ ë¶€ë¶„ì„ ë¯œë¯¸
- kor_ner ë°ì´í„°ì…‹ì˜ ê°œì²´ëª…ì€ ì•„ë˜ì™€ ê°™ìŒ

![img](../../../assets/img/p-stage/klue_07_06.PNG)

<br/>
<div align="right">
    <b><a href="#7ê°•-bert-ì–¸ì–´ëª¨ë¸-ê¸°ë°˜ì˜-ë¬¸ì¥-í† í°-ë¶„ë¥˜">â†¥ back to top</a></b>
</div>
<br/>


## 2. ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹¤ìŠµ

### 2.1 ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
- ì£¼ì˜í•  ì ! í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í°ì„ ìŒì ˆ ë‹¨ìœ„ì˜ í† í°ìœ¼ë¡œ ë¶„í•´í•˜ê³ , Entity tag ì—­ì‹œ ìŒì ˆ ë‹¨ìœ„ë¡œ ë§¤í•‘ì‹œì¼œ ì£¼ì–´ì•¼ í•œë‹¤

![img](../../../assets/img/p-stage/klue_07_07.PNG)

### ë¬¸ì¥ í† í° ë‹¨ìœ„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

```python
$ git clone https://github.com/kmounlp/NER.git

import os
import glob

file_list = []
for x in os.walk('/content/NER/'):
    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):    # ner.*, *_NER.txt
        file_list.append(y)
file_list = sorted(file_list)
```
```
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/00002_NER.txt
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/00003_NER.txt
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/00004_NER.txt
...
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/32765_NER.txt
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/32766_NER.txt
/content/NER/ë§ë­‰ì¹˜ - í˜•íƒœì†Œ_ê°œì²´ëª…/32767_NER.txt
```

- Data Sample
    - ë‚˜ëˆ ì ¸ìˆëŠ” ë¬¸ì¥ì„ í•©ì¹˜ê³  í† í° ë³„ tagë¥¼ labelë¡œ ë§Œë“¤ì–´ ì¤˜ì•¼ í•œë‹¤.

```python
from pathlib import Path

file_path = file_list[0]
file_path = Path(file_path)
raw_text = file_path.read_text().strip()
print(raw_text)
```
```
## 1
## ì˜¤ì— ê²ìë¶€ë¡œëŠ” ì¼ë³¸ í˜„ëŒ€ë¬¸í•™ì˜ ì´ˆì„ì„ ë†“ì€ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì‘ê°€ ë‚˜ì“°ë©” ì†Œì„¸í‚¤(1867~1916)ì˜ ëŒ€í‘œì‘ â€˜ë§ˆìŒâ€™ì— ë‹´ê¸´ êµ°êµ­ì£¼ì˜ì  ìš”ì†Œ, ì•¼ìŠ¤ì¿ ë‹ˆ ì‹ ì‚¬ ì°¸ë°° í–‰ìœ„ê¹Œì§€ ì†Œì„¤ì˜ ì‚½í™”ë¡œ ë™ì›í•˜ë©° ì¼ë³¸ ì‚¬íšŒì˜ â€˜ë¹„ì •ìƒì„±â€™ì„ ë¬¸ì œ ì‚¼ëŠ”ë‹¤.
## <ì˜¤ì— ê²ìë¶€ë¡œ:PER>ëŠ” <ì¼ë³¸:LOC> í˜„ëŒ€ë¬¸í•™ì˜ ì´ˆì„ì„ ë†“ì€ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì‘ê°€ <ë‚˜ì“°ë©” ì†Œì„¸í‚¤:PER>(<1867~1916:DUR>)ì˜ ëŒ€í‘œì‘ â€˜<ë§ˆìŒ:POH>â€™ì— ë‹´ê¸´ êµ°êµ­ì£¼ì˜ì  ìš”ì†Œ, <ì•¼ìŠ¤ì¿ ë‹ˆ ì‹ ì‚¬:ORG> ì°¸ë°° í–‰ìœ„ê¹Œì§€ ì†Œì„¤ì˜ ì‚½í™”ë¡œ ë™ì›í•˜ë©° <ì¼ë³¸:ORG> ì‚¬íšŒì˜ â€˜ë¹„ì •ìƒì„±â€™ì„ ë¬¸ì œ ì‚¼ëŠ”ë‹¤.
ì˜¤ì—	ì˜¤ì—	NNG	B-PER
_	_	_	I-PER
ê²ìë¶€ë¡œ	ê²ìë¶€ë¡œ	NNP	I-PER
ëŠ”	ëŠ”	JX	O
_	_	_	O
ì¼ë³¸	ì¼ë³¸	NNP	B-LOC
_	_	_	O
í˜„ëŒ€	í˜„ëŒ€	NNG	O
ë¬¸í•™	ë¬¸í•™	NNG	O
ì˜	ì˜	JKG	O
_	_	_	O
ì´ˆì„	ì´ˆì„	NNG	O
ì„	ì„	JKO	O
_	_	_	O
ë†“	ë†“	VV	O
ì€	ì€	ETM	O
_	_	_	O
ê²ƒ	ê²ƒ	NNB	O
ìœ¼ë¡œ	ìœ¼ë¡œ	JKB	O
_	_	_	O
í‰ê°€	í‰ê°€	NNG	O
ë°›	ë°›	VV	O
ëŠ”	ëŠ”	ETM	O
_	_	_	O
ì‘ê°€	ì‘ê°€	NNG	O
_	_	_	O
ë‚˜ì“°ë©”	ë‚˜ì“°ë©”	NNP	B-PER
_	_	_	I-PER
ì†Œì„¸í‚¤	ì†Œì„¸í‚¤	NNP	I-PER
(	(	SS	O
1867	1867	SN	B-DUR
~	~	SO	I-DUR
1916	1916	SN	I-DUR
...
```

- ë°ì´í„° ì…‹ ì „ì²˜ë¦¬

```python
def read_file(file_list):
    ...
    # íŒŒì¼ ìˆœíšŒ
    for file_path in file_list:
        ...
        raw_text = file_path.read_text().strip()
        raw_docs = re.split(r'\n\t?\n', raw_text)
        # docs ë³„ë¡œ
        for doc in raw_docs:
            ...
            for line in doc.split('\n'):
                if line[0:1] == "$" or line[0:1] == ";" or line[0:2] == "##":
                    continue
                try:
                    token = line.split('\t')[0]
                    tag = line.split('\t')[3]   # 2: pos, 3: ner
                    for i, syllable in enumerate(token):    # ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ
                        tokens.append(syllable)
                        modi_tag = tag
                        if i > 0:
                            if tag[0] == 'B':
                                modi_tag = 'I' + tag[1:]    # BIO tagë¥¼ ë¶€ì°©í• ê²Œìš” :-)
                        tags.append(modi_tag)
                except:
                    print(line)
            ...
    return token_docs, tag_docs


texts, tags = read_file(file_list[:])
print(texts[0], end='\n\n') # ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ ì˜ë ¸ë„¤ìš”!
print(tags[0])
```
```
['ì˜¤', 'ì—', '_', 'ê²', 'ì', 'ë¶€', 'ë¡œ', 'ëŠ”', '_', 'ì¼', 'ë³¸', '_', 'í˜„', 'ëŒ€', 'ë¬¸', 'í•™', 'ì˜', '_', 'ì´ˆ', 'ì„', 'ì„', '_', 'ë†“', 'ì€', '_', 'ê²ƒ', 'ìœ¼', 'ë¡œ', '_', 'í‰', 'ê°€', 'ë°›', 'ëŠ”', '_', 'ì‘', 'ê°€', '_', 'ë‚˜', 'ì“°', 'ë©”', '_', 'ì†Œ', 'ì„¸', 'í‚¤', '(', '1', '8', '6', '7', '~', '1', '9', '1', '6', ')', 'ì˜', '_', 'ëŒ€', 'í‘œ', 'ì‘', '_', 'â€˜', 'ë§ˆ', 'ìŒ', 'â€™', 'ì—', '_', 'ë‹´', 'ê¸´', '_', 'êµ°', 'êµ­', 'ì£¼', 'ì˜', 'ì ', '_', 'ìš”', 'ì†Œ', ',', '_', 'ì•¼', 'ìŠ¤', 'ì¿ ', 'ë‹ˆ', '_', 'ì‹ ', 'ì‚¬', '_', 'ì°¸', 'ë°°', '_', 'í–‰', 'ìœ„', 'ê¹Œ', 'ì§€', '_', 'ì†Œ', 'ì„¤', 'ì˜', '_', 'ì‚½', 'í™”', 'ë¡œ', '_', 'ë™', 'ì›', 'í•˜', 'ë©°', '_', 'ì¼', 'ë³¸', '_', 'ì‚¬', 'íšŒ', 'ì˜', '_', 'â€˜', 'ë¹„', 'ì •', 'ìƒ', 'ì„±', 'â€™', 'ì„', '_', 'ë¬¸', 'ì œ', '_', 'ì‚¼', 'ëŠ”', 'ë‹¤', '.']

['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

- í•™ìŠµì„ ìœ„í•´ tagë“¤ì„ ìˆ«ìë¡œ ë³€í™˜

```python
unique_tags = set(tag for doc in tags for tag in doc)
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}
```

- ê° NER íƒœê·¸ë³„ ë°ì´í„°ì— í¬í•¨ëœ ê°¯ìˆ˜

```
I-ORG :  41,320
    O : 983,746
B-ORG :  13,089
B-DAT :   5,383
B-PNT :   1,672
B-TIM :     371
B-POH :   6,686
I-DUR :   4,573
I-PNT :   4,613
I-NOH :  23,967
I-PER :  26,206
B-NOH :  11,051
I-MNY :   6,930
I-DAT :  14,433
B-LOC :   6,313
B-PER :  13,779
I-POH :  37,156
I-TIM :   1,876
B-DUR :   1,207
I-LOC :  16,537
B-MNY :   1,440
```

- Train:Test ë¹„ìœ¨ì€ 8:2
- `MODEL_NAME = "bert-base-multilingual-cased"`
- special tokens

```python
pad_token_id = tokenizer.pad_token_id # 0
cls_token_id = tokenizer.cls_token_id # 101
sep_token_id = tokenizer.sep_token_id # 102
pad_token_label_id = tag2id['O']    # tag2id['O']
cls_token_label_id = tag2id['O']
sep_token_label_id = tag2id['O']
```

- Char-based tokenizer
    - ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„! tokenë“¤ì´ ì–´ë–»ê²Œ ì •ì˜ë˜ì–´ ìˆë‚˜ í™•ì¸ ì˜ í•˜ê¸°

```python
def ner_tokenizer(sent, max_seq_length):    
    pre_syllable = "_"
    input_ids = [pad_token_id] * (max_seq_length - 1)
    attention_mask = [0] * (max_seq_length - 1)
    token_type_ids = [0] * max_seq_length
    sent = sent[:max_seq_length-2]

    for i, syllable in enumerate(sent):
        if syllable == '_':
            pre_syllable = syllable
        if pre_syllable != "_":
            syllable = '##' + syllable  # ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefixë¥¼ ë¶™ì…ë‹ˆë‹¤.
            # ì´ìˆœì‹ ì€ ì¡°ì„  -> [ì´, ##ìˆœ, ##ì‹ , ##ì€, ì¡°, ##ì„ ]
        pre_syllable = syllable

        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))
        attention_mask[i] = 1

    input_ids = [cls_token_id] + input_ids
    input_ids[len(sent)+1] = sep_token_id
    attention_mask = [1] + attention_mask
    attention_mask[len(sent)+1] = 1
    return {"input_ids":input_ids,
            "attention_mask":attention_mask,
            "token_type_ids":token_type_ids}
```

- training label ì œì‘

```python
def encode_tags(tags, max_seq_length, pad_token_id=1):
    tags = tags[:max_seq_length-2]
    labels = [tag2id['O']] + [tag2id[tag] for tag in tags]

    padding_length = max_seq_length - len(labels)
    labels = labels + ([pad_token_id] * padding_length)

    return labels
```

- ëª¨ë¸ í˜¸ì¶œ

```python
model = BertForTokenClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(unique_tags)
 )
```

- New Data Inference

```python
def ner_inference(text) :

    model.eval()
    text = text.replace(' ', '_')

    predictions , true_labels = [], []

    tokenized_sent = ner_tokenizer(text, len(text)+2)
    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)
    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)
    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

    logits = outputs['logits']
    logits = logits.detach().cpu().numpy()
    label_ids = token_type_ids.cpu().numpy()

    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
    true_labels.append(label_ids)

    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]

    print('{}\t{}'.format("TOKEN", "TAG"))
    print("===========")
    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):
    #   print("{:^5}\t{:^5}".format(token, tag))
    for i, tag in enumerate(pred_tags):
        print("{:^5}\t{:^5}".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))


text = 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'
ner_inference(text)
```
```
TOKEN	TAG
===========
[CLS]	  O  
  ì´  	B-PER
 ##ìˆœ 	I-PER
 ##ì‹  	I-PER
 ##ì€ 	  O  
  _  	  O  
  ì¡°  	  O  
 ##ì„  	  O  
  _  	  O  
  ì¤‘  	  O  
 ##ê¸° 	  O  
 ##ì˜ 	  O  
  _  	  O  
  ë¬´  	  O  
 ##ì‹  	  O  
 ##ì´ 	  O  
 ##ë‹¤ 	  O  
 ##. 	  O  
[SEP]	  O  
```
<br/>
<div align="right">
    <b><a href="#7ê°•-bert-ì–¸ì–´ëª¨ë¸-ê¸°ë°˜ì˜-ë¬¸ì¥-í† í°-ë¶„ë¥˜">â†¥ back to top</a></b>
</div>
<br/>


### ê¸°ê³„ ë…í•´ ëª¨ë¸ í•™ìŠµ

- Dataset Download

```python
!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json
!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json
```

- Dataset Parsing

```python
# ë°ì´í„°ì…‹ì„ íŒŒì‹±í•´ì˜¤ê² ìŠµë‹ˆë‹¤ :-)

import json
from pathlib import Path

def read_squad(path):
    path = Path(path)
    with open(path, 'rb') as f:
        squad_dict = json.load(f)

    contexts = []
    questions = []
    answers = []
    for group in squad_dict['data']:
        for passage in group['paragraphs']:
            context = passage['context']
            for qa in passage['qas']:
                question = qa['question']
                for answer in qa['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)

    return contexts, questions, answers


train_contexts, train_questions, train_answers = read_squad('dataset/KorQuAD_v1.0_train.json')
val_contexts, val_questions, val_answers = read_squad('dataset/KorQuAD_v1.0_dev.json')
```
```
Context
===================================================
ì‚¬ìš°ìŠ¤ëŸ°ë˜ ë¸Œë¦­ìŠ¤í†¤ íƒœìƒ ë³´ìœ„ëŠ” ì–´ë¦´ ì  ìŒì•…ì— ëŒ€í•œ ê´€ì‹¬ì„ í‚¤ì›Œì™”ìœ¼ë©°, ê²°êµ­ì—ëŠ” ì˜ˆìˆ , ìŒì•…, ë””ìì¸ì„ ë°°ì›Œ ì „ë¬¸ì ì¸ ìŒì•…ê°€ ê²½ë ¥ì„ 1963ë…„ë¶€í„° ì°©ìˆ˜í–ˆë‹¤.
ã€ˆSpace Oddityã€‰ëŠ” 1969ë…„ 7ì›” ë°œí‘œ ë’¤ ì˜êµ­ ìŒë°˜ ì°¨íŠ¸ì—ì„œ
ìƒìœ„ 5ìœ„ì— ì˜¤ë¥¸ ê·¸ì˜ ì²« ì‹±ê¸€ì´ë‹¤.
ì‹¤í—˜ í™œë™ì„ ê±°ì¹œ ê·¸ëŠ” 1972ë…„ ìŒì•…ì„ ì¬ê°œ, ìì‹ ì˜ ê¸€ë¨ë¡ ì‹œê¸° ë™ì•ˆ ì´ìƒ‰ì ì´ê³  ì–‘ì„±ì ì¸ ì œ2ì˜ ìì•„ì¸ ì§€ê¸° ìŠ¤íƒ€ë”ìŠ¤íŠ¸ë¡œ í™œë™ì„ ì´ì–´ë‚˜ê°”ë‹¤.
ì„±ê³µì„ ê±°ë‘” ì‹±ê¸€ ã€ˆStarmanã€‰ê³¼ ì „ ì„¸ê³„ì ì¸ ì¸ê¸°ë¥¼ ëˆ ìŒë°˜ ã€ŠThe Rise and Fall of Ziggy Stardust and the Spiders from Marsã€‹ìœ¼ë¡œ ìºë¦­í„°ë¥¼ ë‚´ì„¸ìš´ ë³´ìœ„ëŠ” 1975ë…„ "í”Œë¼ìŠ¤í‹± ì†”"ì„ ìºë¦­í„°í™”ì‹œì¼œ ìì‹ ì„ ì² ì €íˆ ë°”ê¾¼ë‹¤.
ì´ í–‰ë™ì€ ë‹¹ì´ˆ ì˜êµ­ì—ì„œ ê·¸ì˜ ì—´í˜ˆíŒ¬ì˜ ë°˜ë°œì„ ìƒ€ìœ¼ë‚˜ ë¯¸êµ­ì—ì„œëŠ” ì‹±ê¸€ ã€ˆFameã€‰ê³¼ ìŒë°˜ ã€ŠYoung Americansã€‹ì„ í†µí•´ ì²˜ìŒìœ¼ë¡œ ë©”ì´ì €í•œ ì„±ê³µì„ ê±°ë‘ê²Œ ëœë‹¤.
1976ë…„ ë³´ìœ„ëŠ” ì»¬íŠ¸ ì˜í™” ã€Šì§€êµ¬ì— ë–¨ì–´ì§„ ì‚¬ë‚˜ì´ã€‹ì— ì¶œì—°í•˜ê³  ìŒë°˜ ã€ŠStation to Stationã€‹ì„ ë°œí‘œí•œë‹¤.
ì´ë“¬í•´ì—ëŠ” ì¼ë ‰íŠ¸ë¦­ ìŒì•…ì„ ì ‘ëª©í•œ ìŒë°˜ ã€ŠLowã€‹ (1977)ì„ ë°œí‘œí•˜ë©´ì„œ ìŒì•…ì  ì˜ˆìƒì„ ê¹¨ëœ¨ë ¸ë‹¤.
ì´ ìŒë°˜ì€ ë¸Œë¼ì´ì–¸ ì´ë…¸ì™€ì˜ ì„¸ ë²ˆì˜ í˜‘ì—… ì¤‘ ì²« ë²ˆì§¸ë¡œ ì´ëŠ” ì´í›„ "ë² ë¥¼ë¦° ì‚¼ë¶€ì‘"ìœ¼ë¡œ ì¼ì»¬ì–´ì§„ë‹¤.
ë’¤ë¥¼ ì´ì–´ ë°œí‘œëœ ã€Š"Heroes"ã€‹ (1977)ì™€ ã€ŠLodgerã€‹ (1979)ëŠ” ì˜êµ­ ì°¨íŠ¸ ìƒìœ„ 5ìœ„ì— ì§„ì…, ì§€ì†ì ì¸ ê·¹ì°¬ì„ ë°›ì•˜ë‹¤.

Question
===================================================
ë³´ìœ„ê°€ 1977ë…„ ì¼ë ‰íŠ¸ë¦­ ìŒì•…ì„ ì ‘ëª©í•˜ì—¬ ë°œí‘œí•œ ìŒë°˜ì€?

Answer
===================================================
{'text': 'Low', 'answer_start': 568}
```

- Add end idx
    - answerì˜ ê¸¸ì´ë¡œ get

```python
def add_end_idx(answers, contexts):
    for answer, context in zip(answers, contexts):
        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë°ì´í„°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.
        # ì •ë‹µ ë°ì´í„°ëŠ” startìŒì ˆê³¼ end ìŒì ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        # ëª¨ë¸ì€ ì „ì²´ í† í° ì¤‘ì—ì„œ start tokenê³¼ end tokenì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
        gold_text = answer['text']
        start_idx = answer['answer_start']
        end_idx = start_idx + len(gold_text)


        # sometimes squad answers are off by a character or two â€“ fix this
        # ì‹¤ì œ ë³¸ë¬¸ì—ì„œ í•´ë‹¹ ìŒì ˆ ë²ˆí˜¸ë¡œ ì˜ë¼ëƒˆì„ ë•Œ, ì •ë‹µê³¼ ê°™ì€ì§€ ê²€ì‚¬í•´ì„œ start, endë¥¼ ë³´ì •í•©ë‹ˆë‹¤ :-)
        # 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤' -> 'ì´ìˆœì‹ ' -> start: 0, end: 4
        if context[start_idx:end_idx] == gold_text:
            answer['answer_end'] = end_idx
        elif context[start_idx-1:end_idx-1] == gold_text:
            answer['answer_start'] = start_idx - 1
            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character
        elif context[start_idx-2:end_idx-2] == gold_text:
            answer['answer_start'] = start_idx - 2
            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters
    return answers

train_answers = add_end_idx(train_answers, train_contexts)
val_answers = add_end_idx(val_answers, val_contexts)
```

- Token position ì¶”ê°€

```python
def add_token_positions(encodings, answers):
    start_positions = []
    end_positions = []
    # ì´ì œ ìŒì ˆ indexë¥¼ token indexì™€ mappingí•˜ëŠ” ì‘ì—…ì„ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ :-)
    for i in range(len(answers)):
        # tokenizerì˜ char_to_token í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))
        # ì•„ë˜ ë¶€ë¶„ì€ truncationì„ ìœ„í•œ ê³¼ì •ì…ë‹ˆë‹¤.
        # if start position is None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length

        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1
        if end_positions[-1] is None:
            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)

        # ì¶”ê°€ëœ ì˜ˆì™¸ ì²˜ë¦¬, ì˜ˆë¥¼ë“¤ì–´ì„œ tokenizerì™€ model inputì˜ max_lengthê°€ 512ì¸ë°, startì™€ end positionì´ 600ê³¼ 610 ì´ë©´ ë‘˜ë‹¤ max_lengthë¡œ ë³€ê²½í•´ì•¼í•¨.
        # ì–´ì°¨í”¼ max_lengthê°€ 512ì¸ ëª¨ë¸ì€ ì •ë‹µì„ ë³¼ ìˆ˜ ì—†ìŒ.
        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length:
            start_positions[-1] = tokenizer.model_max_length

        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})
    return encodings

train_encodings = add_token_positions(train_encodings, train_answers)
val_encodings = add_token_positions(val_encodings, val_answers)
```

- ëª¨ë¸ í˜¸ì¶œ ë° í•™ìŠµ

```python
model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)
model.to(device)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset            # evaluation dataset
)
```

- ê²°ê³¼ í™•ì¸

```python
nlp = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0)

context = r"""
ì´ìˆœì‹ (æèˆœè‡£, 1545ë…„ 4ì›” 28ì¼ ~ 1598ë…„ 12ì›” 16ì¼ (ìŒë ¥ 11ì›” 19ì¼))ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ì—ˆë‹¤.
ë³¸ê´€ì€ ë•ìˆ˜(å¾·æ°´), ìëŠ” ì—¬í•´(æ±è«§), ì‹œí˜¸ëŠ” ì¶©ë¬´(å¿ æ­¦)ì˜€ìœ¼ë©°, í•œì„± ì¶œì‹ ì´ì—ˆë‹¤.
ë¬¸ë°˜ ê°€ë¬¸ ì¶œì‹ ìœ¼ë¡œ 1576ë…„(ì„ ì¡° 9ë…„) ë¬´ê³¼(æ­¦ç§‘)ì— ê¸‰ì œí•˜ì—¬ ê·¸ ê´€ì§ì´ ë™êµ¬ë¹„ë³´ ê¶Œê´€, í›ˆë ¨ì› ë´‰ì‚¬, ë°œí¬ì§„ ìˆ˜êµ°ë§Œí˜¸, ì¡°ì‚°ë³´ ë§Œí˜¸, ì „ë¼ì¢Œë„ ìˆ˜êµ°ì ˆë„ì‚¬ë¥¼ ê±°ì³ ì •í—ŒëŒ€ë¶€ ì‚¼ë„ìˆ˜êµ°í†µì œì‚¬ì— ì´ë¥´ë €ë‹¤.
"""

print(nlp(question="ì´ìˆœì‹ ì´ íƒœì–´ë‚œ ë‚ ì§œëŠ”?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ë³¸ê´€ì€?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ì‹œí˜¸ëŠ”?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ê³ í–¥ì€?", context=context))
print(nlp(question="ì´ìˆœì‹ ì˜ ë§ˆì§€ë§‰ ì§ì±…ì€?", context=context))
```
```
{'score': 0.1287170797586441, 'start': 25, 'end': 40, 'answer': '1598ë…„ 12ì›” 16ì¼ ('}
{'score': 0.5768629908561707, 'start': 72, 'end': 75, 'answer': 'ë•ìˆ˜('}
{'score': 0.4932706952095032, 'start': 95, 'end': 98, 'answer': 'ì¶©ë¬´('}
{'score': 0.1482970416545868, 'start': 106, 'end': 114, 'answer': 'í•œì„± ì¶œì‹ ì´ì—ˆë‹¤'}
{'score': 0.038699887692928314, 'start': 214, 'end': 222, 'answer': 'ì‚¼ë„ìˆ˜êµ°í†µì œì‚¬ì—'}
```

<br/>
<div align="right">
    <b><a href="#7ê°•-bert-ì–¸ì–´ëª¨ë¸-ê¸°ë°˜ì˜-ë¬¸ì¥-í† í°-ë¶„ë¥˜">â†¥ back to top</a></b>
</div>
<br/>

## Reference
ê°œì²´ëª… ì¸ì‹
- [Named Entity Recognition (NER) for Turkish with BERT](https://medium.com/analytics-vidhya/named-entity-recognition-for-turkish-with-bert-f8ec04a31b0)

QA
- [lonformer_qa_training.ipynb](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks ](https://jeonsworld.github.io/NLP/rag/)

BERT seq2seq
- [BERT2BERT_for_CNN_Dailymail.ipynb](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
- [Bert2Bert Summarization](https://github.com/MrBananaHuman/bert2bert-summarization)


```python

```

[link]()
![img]()


<br/>
<div align="right">
    <b><a href="#7ê°•-bert-ì–¸ì–´ëª¨ë¸-ê¸°ë°˜ì˜-ë¬¸ì¥-í† í°-ë¶„ë¥˜">â†¥ back to top</a></b>
</div>
<br/>
