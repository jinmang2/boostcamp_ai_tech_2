# 4ê°• í•œêµ­ì–´ BERT ì–¸ì–´ ëª¨ë¸ í•™ìŠµ

ì´ë²ˆì—ëŠ” 3ê°•ì—ì„œ ì†Œê°œí•œ BERTë¥¼ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê°•ì˜ì…ë‹ˆë‹¤.

ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ë“¤ì„ í™œìš©í•˜ê³  ê³µìœ í•  ìˆ˜ ìˆëŠ” Huggingface Hubì— ëŒ€í•´ ì†Œê°œí•˜ê³ , ì§ì ‘ ë³¸ì¸ì˜ ëª¨ë¸ì„ ê³µìœ í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ğŸ¤“

<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#1-bert-í•™ìŠµí•˜ê¸°">BERT í•™ìŠµí•˜ê¸°</a>
      <ul>
        <li><a href="#11-bert-ëª¨ë¸-í•™ìŠµ">BERT ëª¨ë¸ í•™ìŠµ</a></li>
      </ul>
    </li>
    <li><a href="#bert-mask-token-ê³µê²©">BERT [MASK] token ê³µê²©!</a></li>
    <li><a href="#í•œêµ­ì–´-bert-ëª¨ë¸-í•™ìŠµ">í•œêµ­ì–´ BERT ëª¨ë¸ í•™ìŠµ</a></li>
    <li><a href="#referece">Reference</a></li>
  </ol>
</details>

## 1. BERT í•™ìŠµí•˜ê¸°

### 1.1 BERT ëª¨ë¸ í•™ìŠµ
- ì´ë¯¸ ìˆëŠ” ê±° ì“°ì§€, ì™œ ìƒˆë¡œ í•™ìŠµí•´ì•¼  í•˜ë‚˜ìš”?
- Domain-specificí•œ ê²½ìš°, ë„ë©”ì¸ íŠ¹í™”ëœ í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ

![img](../../../assets/img/p-stage/klue_04_01.PNG)

- ìƒë¦¬í•™ ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ scratchë¡œ í•™ìŠµí•œ ê²ƒì´ domain íŠ¹í™”ì—ì„  ë” ì¢‹ìŒ

![img](../../../assets/img/p-stage/klue_04_02.PNG)

<br/>
<div align="right">
    <b><a href="#4ê°•-í•œêµ­ì–´-bert-ì–¸ì–´-ëª¨ë¸-í•™ìŠµ">â†¥ back to top</a></b>
</div>
<br/>

## BERT [MASK] token ê³µê²©!

![img](../../../assets/img/p-stage/klue_04_03.PNG)

```python
$ pip install transformers

from transformers import BertForMaskedLM, AutoTokenizer
# Store the model we want to use
MODEL_NAME = "bert-base-multilingual-cased"

# We need to create the model and tokenizer
model = BertForMaskedLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

from transformers import pipeline

nlp_fill = pipeline('fill-mask', top_k=5, model=model, tokenizer=tokenizer)
nlp_fill('Martin is living in [MASK].')
```

- ë²„ë½ ì˜¤ë°”ë§ˆì˜ ì •ë³´ë¥¼ ì–»ì–´ë³´ì

```python
nlp_fill('Barack Hussein Obama graduated from [MASK] University.')
```
```
[{'score': 0.07715228945016861,
  'sequence': 'Barack Hussein Obama graduated from Harvard University.',
  'token': 16744,
  'token_str': 'Harvard'},
 {'score': 0.04509279131889343,
  'sequence': 'Barack Hussein Obama graduated from Georgetown University.',
  'token': 57543,
  'token_str': 'Georgetown'},
 {'score': 0.028830749914050102,
  'sequence': 'Barack Hussein Obama graduated from Northwestern University.',
  'token': 78396,
  'token_str': 'Northwestern'},
 {'score': 0.02869964763522148,
  'sequence': 'Barack Hussein Obama graduated from Lincoln University.',
  'token': 16944,
  'token_str': 'Lincoln'},
 {'score': 0.02225659042596817,
  'sequence': 'Barack Hussein Obama graduated from Boston University.',
  'token': 13683,
  'token_str': 'Boston'}]
```

```python
nlp_fill('Obama was [MASK] of the United States.')
```
```
[{'score': 0.48057445883750916,
  'sequence': 'Obama was President of the United States.',
  'token': 12811,
  'token_str': 'President'},
 {'score': 0.05541132390499115,
  'sequence': 'Obama was president of the United States.',
  'token': 12931,
  'token_str': 'president'},
 {'score': 0.044774625450372696,
  'sequence': 'Obama was War of the United States.',
  'token': 11277,
  'token_str': 'War'},
 {'score': 0.02629113756120205,
  'sequence': 'Obama was part of the United States.',
  'token': 10668,
  'token_str': 'part'},
 {'score': 0.02169841341674328,
  'sequence': 'Obama was States of the United States.',
  'token': 10859,
  'token_str': 'States'}]
```

```python
nlp_fill('Obama was born in [MASK], Hawaii.')
```
```
[{'score': 0.8043831586837769,
  'sequence': 'Obama was born in Honolulu, Hawaii.',
  'token': 56348,
  'token_str': 'Honolulu'},
 {'score': 0.16054971516132355,
  'sequence': 'Obama was born in Hawaii, Hawaii.',
  'token': 21729,
  'token_str': 'Hawaii'},
 {'score': 0.001997528364881873,
  'sequence': 'Obama was born in County, Hawaii.',
  'token': 10886,
  'token_str': 'County'},
 {'score': 0.001665070652961731,
  'sequence': 'Obama was born in Mesa, Hawaii.',
  'token': 25076,
  'token_str': 'Mesa'},
 {'score': 0.0008855816558934748,
  'sequence': 'Obama was born in Kai, Hawaii.',
  'token': 26387,
  'token_str': 'Kai'}]
```

<br/>
<div align="right">
    <b><a href="#4ê°•-í•œêµ­ì–´-bert-ì–¸ì–´-ëª¨ë¸-í•™ìŠµ">â†¥ back to top</a></b>
</div>
<br/>

## í•œêµ­ì–´ BERT ëª¨ë¸ í•™ìŠµ

- Tokenizer ë§Œë“¤ê¸°

```python
from tokenizers import BertWordPieceTokenizer

# Initialize an empty tokenizer
wp_tokenizer = BertWordPieceTokenizer(
    clean_text=True,   # ["ì´ìˆœì‹ ", "##ì€", " ", "ì¡°ì„ "] ->  ["ì´ìˆœì‹ ", "##ì€", "ì¡°ì„ "]
    # if char == " " or char == "\t" or char == "\n" or char == "\r":
    handle_chinese_chars=True,  # í•œìëŠ” ëª¨ë‘ char ë‹¨ìœ„ë¡œ ìª¼ê²Œë²„ë¦½ë‹ˆë‹¤.
    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]
    lowercase=False,    # Hello -> hello
)

# And then train
wp_tokenizer.train(
    files="my_data/wiki_20190620_small.txt",
    vocab_size=20000,   # vocab size ë¥¼ ì§€ì •í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    min_frequency=2,
    show_progress=True,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    wordpieces_prefix="##"
)

# Save the files
wp_tokenizer.save_model("wordPieceTokenizer", "my_tokenizer")

tokenizer = BertTokenizerFast(
    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',
    max_len=128,
    do_lower_case=False,
)

tokenizer.add_special_tokens({'mask_token':'[MASK]'})
print(tokenizer.tokenize("ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."))
```

- ëª¨ë¸ ê»ë°ê¸° ê°€ì ¸ì˜¤ê¸°

```python
from transformers import BertConfig, BertForPreTraining

config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig
    vocab_size=20000,
    # hidden_size=512,
    # num_hidden_layers=12,    # layer num
    # num_attention_heads=8,    # transformer attention head number
    # intermediate_size=3072,   # transformer ë‚´ì— ìˆëŠ” feed-forward networkì˜ dimension size
    # hidden_act="gelu",
    # hidden_dropout_prob=0.1,
    # attention_probs_dropout_prob=0.1,
    max_position_embeddings=128,    # embedding size ìµœëŒ€ ëª‡ tokenê¹Œì§€ inputìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì§€ì •
    # type_vocab_size=2,    # token type idsì˜ ë²”ìœ„ (BERTëŠ” segmentA, segmentBë¡œ 2ì¢…ë¥˜)
    # pad_token_id=0,
    # position_embedding_type="absolute"
)

model = BertForPreTraining(config=config)
model.num_parameters()
```

- MLM DataCollator
    - ì™€... ì´ê±¸ ì§€ì›í•´ì£¼ë„¤?

```python
from transformers import DataCollatorForLanguageModeling
```

- Next Sentence Prediction
    - ë‘ ë²ˆ ë‹¤ì‹œ ì•ˆë³¼ê±°ë¼ ì´ë²ˆì— ì—´ì‹¬íˆ ë³´ì

```python
# Google BERTì˜ GitHub ê³µì‹ ì½”ë“œ
class TextDatasetForNextSentencePrediction(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        short_seq_probability=0.1,
        nsp_probability=0.5,
    ):
        # ì—¬ê¸° ë¶€ë¶„ì€ í•™ìŠµ ë°ì´í„°ë¥¼ cachingí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤ :-)
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"

        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)
        self.short_seq_probability = short_seq_probability
        self.nsp_probability = nsp_probability

        directory, filename = os.path.split(file_path)
        cached_features_file = os.path.join(
            directory,
            "cached_nsp_{}_{}_{}".format(
                tokenizer.__class__.__name__,
                str(block_size),
                filename,
            ),
        )

        self.tokenizer = tokenizer

        lock_path = cached_features_file + ".lock"

        # Input file format:
        # (1) One sentence per line. These should ideally be actual sentences, not
        # entire paragraphs or arbitrary spans of text. (Because we use the
        # sentence boundaries for the "next sentence prediction" task).
        # (2) Blank lines between documents. Document boundaries are needed so
        # that the "next sentence prediction" task doesn't span between documents.
        #
        # Example:
        # I am very happy.
        # Here is the second sentence.
        #
        # A new document.

        with FileLock(lock_path):
            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )
            else:
                # ë³¸ê²©ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ê³¼ì •!
                # cacheê°€ ì—†ìœ¼ë©´ ì•„ë˜ë¥¼ ì‹¤í–‰
                logger.info(f"Creating features from dataset file at {directory}")
                self.documents = [[]]
                with open(file_path, encoding="utf-8") as f:
                    """ ì¼ë‹¨ ë¬¸ì¥ì„ ì½ìŒ """
                    while True:
                        line = f.readline()
                        if not line:
                            break
                        line = line.strip()

                        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì €ì¥
                        if not line and len(self.documents[-1]) != 0:
                            self.documents.append([])
                        tokens = tokenizer.tokenize(line)
                        tokens = tokenizer.convert_tokens_to_ids(tokens)
                        if tokens:
                            self.documents[-1].append(tokens)
                # """  """
                logger.info(f"Creating examples from {len(self.documents)} documents.")
                self.examples = []
                # """  """
                for doc_index, document in enumerate(self.documents):
                    self.create_examples_from_document(document, doc_index)

                start = time.time()
                with open(cached_features_file, "wb") as handle:
                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(
                    "Saving features into cached file %s [took %.3f s]", cached_features_file, time.time() - start
                )

    def create_examples_from_document(self, document: List[List[int]], doc_index: int):
        """Creates examples for a single document."""
        # 128 - 2 = 126
        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)

        # We *usually* want to fill up the entire sequence since we are padding
        # to `block_size` anyways, so short sequences are generally wasted
        # computation. However, we *sometimes*
        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
        # sequences to minimize the mismatch between pretraining and fine-tuning.
        # The `target_seq_length` is just a rough target however, whereas
        # `block_size` is a hard limit.

        # 2~126ì˜ tokenë“¤ë§Œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©
        target_seq_length = max_num_tokens
        if random.random() < self.short_seq_probability:
            target_seq_length = random.randint(2, max_num_tokens)

        current_chunk = []  # a buffer stored current working segments
        current_length = 0
        i = 0

        # sent+..+sent [SEP] sent+...+sent
        # ê¸°ì¤€ì€ target_seq_lenght
        while i < len(document):
            segment = document[i]
            current_chunk.append(segment)
            current_length += len(segment)
            if i == len(document) - 1 or current_length >= target_seq_length:
                if current_chunk:
                    # `a_end` is how many segments from `current_chunk` go into the `A`
                    # (first) sentence.
                    a_end = 1
                    # If sent+sent before [SEP],
                    # ê¸¸ì´ë¥¼ randomí•˜ê²Œ ìë¦„
                    if len(current_chunk) >= 2:
                        a_end = random.randint(1, len(current_chunk) - 1)
                    tokens_a = []
                    for j in range(a_end):
                        tokens_a.extend(current_chunk[j])
                    # ì´ì œ [SEP] ë’· ë¶€ë¶„ì¸ segmentBë¥¼ ì‚´í´ë³¼ê¹Œìš”?
                    tokens_b = []
                    # 50%ì˜ í™•ë¥ ë¡œ ëœë¤í•˜ê²Œ ë‹¤ë¥¸ ë¬¸ì¥ì„ ì„ íƒí•˜ê±°ë‚˜, ë‹¤ìŒ ë¬¸ì¥ì„ í•™ìŠµë°ì´í„°ë¡œ ë§Œë“­ë‹ˆë‹¤.
                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:
                        is_random_next = True
                        target_b_length = target_seq_length - len(tokens_a)

                        # This should rarely go for more than one iteration for large
                        # corpora. However, just to be careful, we try to make sure that
                        # the random document is not the same as the document
                        # we're processing.
                        for _ in range(10):
                            random_document_index = random.randint(0, len(self.documents) - 1)
                            if random_document_index != doc_index:
                                break
                        # ì—¬ê¸°ì„œ ëœë¤í•˜ê²Œ ì„ íƒí•©ë‹ˆë‹¤ :-)
                        random_document = self.documents[random_document_index]
                        random_start = random.randint(0, len(random_document) - 1)
                        for j in range(random_start, len(random_document)):
                            tokens_b.extend(random_document[j])
                            if len(tokens_b) >= target_b_length:
                                break
                        # We didn't actually use these segments so we "put them back" so
                        # they don't go to waste.
                        num_unused_segments = len(current_chunk) - a_end
                        i -= num_unused_segments
                    # Actual next
                    else:
                        is_random_next = False
                        for j in range(a_end, len(current_chunk)):
                            tokens_b.extend(current_chunk[j])

                    # 126 tokenì„ ê¸°ì¤€ìœ¼ë¡œ truncation
                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):
                        """Truncates a pair of sequences to a maximum sequence length."""
                        while True:
                            total_length = len(tokens_a) + len(tokens_b)
                            if total_length <= max_num_tokens:
                                break
                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
                            assert len(trunc_tokens) >= 1
                            # We want to sometimes truncate from the front and sometimes from the
                            # back to add more randomness and avoid biases.
                            if random.random() < 0.5:
                                del trunc_tokens[0]
                            else:
                                trunc_tokens.pop()

                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)

                    assert len(tokens_a) >= 1
                    assert len(tokens_b) >= 1

                    # add special tokens
                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)
                    # add token type ids, 0 for sentence a, 1 for sentence b
                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)

                    example = {
                        "input_ids": torch.tensor(input_ids, dtype=torch.long),
                        "token_type_ids": torch.tensor(token_type_ids, dtype=torch.long),
                        "next_sentence_label": torch.tensor(1 if is_random_next else 0, dtype=torch.long),
                    }

                    self.examples.append(example)

                current_chunk = []
                current_length = 0

            i += 1

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return self.examples[i]
```

- DataCollatorë¡œ ë§ˆìŠ¤í‚¹

```python
tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())
```
```
[CLS] [MASK] ì–¼ " ì§€ë¯¸ " ì¹´í„° ì£¼ë‹ˆ [MASK] ë¯¼ì£¼ë‹¹ ì¶œì‹  ë¯¸êµ­ 39ë²ˆì§¸ ëŒ€í†µë ¹ ì´ë‹¤ [MASK] ì§€ë¯¸ [MASK] [MASK] ì„¬í„° ì¹´ [MASK]í‹° í”Œë ˆì¸ìŠ¤ ë§ˆì„ì—ì„œ íƒœì–´ë‚¬ë‹¤. ì¡°ì§€ì•„ ê³µê³¼ëŒ€í•™êµë¥¼ ì¡¸ì—…í•˜ì˜€ë‹¤. [SEP] ì£¼ì–´ì§„ ë‘ ë²¡í„° [MASK] ì‚¬ì´ì˜ ìœ ê°€ì™€ ë³€í™˜ì˜ ì§‘í•©ì€ ì ë³„ ë²¡í„° ë§ì…ˆê³¼ ì ë³„ ìŠ¤ì¹¼ë¼ ê³±ì…ˆì— ì˜í•˜ì—¬ ë²¡í„° ê³µê°„ì„ ì´ë£¬ë‹¤. ë‘ ìœ í•œ ì°¨ì› ë²¡í„° ê³µê°„ ì‚¬ì´ì˜ ì„ í˜• [MASK] [MASK] ê¸°ì €ì— ëŒ€í•œ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì„ íƒ ê³µë¦¬ë¥¼ ê°€ì •í•˜ì. ì²´ formula _ 1ì— ëŒ€í•œ ë²¡í„° ê³µê°„ formula _ 4ì— ê¸°ê³„ì œ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤. ì¦‰, ì£¼ì–´ì§„ ì²´ì— ëŒ€í•œ ë²¡í„° ê³µê°„ì€ ê·¸ ì°¨ì›ì— ë”°ë¼ì„œ ì™„ì „íˆ ë¶„ë¥˜ëœë‹¤. ì´ëŠ” ì„ íƒ ê³µë¦¬ë¥¼ í•„ìš”ë¡œ í•˜ë©°, ì„ íƒ ê³µë¦¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë²¡í„° [MASK] ì°¨ì›ì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ì—†ë‹¤ [MASK] [SEP]
```

- ê²°ê³¼ í…ŒìŠ¤íŠ¸

```python
from transformers import BertForMaskedLM, pipeline

my_model = BertForMaskedLM.from_pretrained('model_output')

nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)

nlp_fill('ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.')
```
```

[{'score': 0.030770400539040565,
  'sequence': '[CLS] ì´ìˆœì‹ ì€, ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]',
  'token': 14,
  'token_str': ','},
 {'score': 0.03006444126367569,
  'sequence': '[CLS] ì´ìˆœì‹ ì€. ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]',
  'token': 16,
  'token_str': '.'},
 {'score': 0.012540608644485474,
  'sequence': '[CLS] ì´ìˆœì‹ ì€ _ ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]',
  'token': 63,
  'token_str': '_'},
 {'score': 0.008801406249403954,
  'sequence': '[CLS] ì´ìˆœì‹ ì€ ìˆë‹¤ ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]',
  'token': 1888,
  'token_str': 'ìˆë‹¤'},
 {'score': 0.008582047186791897,
  'sequence': '[CLS] ì´ìˆœì‹ ì€ formula ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]',
  'token': 1895,
  'token_str': 'formula'}]
```

<br/>
<div align="right">
    <b><a href="#4ê°•-í•œêµ­ì–´-bert-ì–¸ì–´-ëª¨ë¸-í•™ìŠµ">â†¥ back to top</a></b>
</div>
<br/>


## Reference
- [LM training from scratch](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw)
- ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°
    - [Wordpiece Vocab ë§Œë“¤ê¸°](https://monologg.kr/2020/04/27/wordpiece-vocab/)
    - [Wordpiece Tokenizer ë§Œë“¤ê¸°](https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0)
- [Extracting training data from large language model](https://www.youtube.com/watch?v=NGoDUEz3tZg)
- [BERT ì¶”ê°€ ì„¤ëª…](https://jiho-ml.com/weekly-nlp-28/)


<br/>
<div align="right">
    <b><a href="#4ê°•-í•œêµ­ì–´-bert-ì–¸ì–´-ëª¨ë¸-í•™ìŠµ">â†¥ back to top</a></b>
</div>
<br/>
