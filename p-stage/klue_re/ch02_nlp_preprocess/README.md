# 2ê°• ìì—°ì–´ì˜ ì „ì²˜ë¦¬

ì¸ê³µì§€ëŠ¥ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„°!! â€œGarbage in, garbage outâ€ ì´ë¼ëŠ” ë§ì´ ìˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•´ì•¼ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë˜í•œ ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë‹¨ìˆœíˆ 'ì •ì œ' ì˜ ê°œë…ì´ ì•„ë‹ˆë¼, ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê² ë‹¤ëŠ” task ì •ì˜ì˜ ì˜ë¯¸ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤! â˜ºï¸
ë°˜ë“œì‹œ ë‚  ê²ƒì˜ ë°ì´í„°ì™€ ì¹œí•˜ê²Œ ì§€ë‚´ì„¸ìš”!!!

ì´ë²ˆ ê°•ì˜ì—ëŠ” ë‹¤ì–‘í•œ í•œêµ­ì–´ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ ê¸°ë²•ì„ ë°°ìš°ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤.ğŸ˜
í•œêµ­ì–´ë¡œ í•  ìˆ˜ ìˆëŠ” ê±°ì˜ ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤! ğŸ˜€

[back to super](https://github.com/jinmang2/boostcamp_ai_tech_2/tree/main/p-stage/klue_re)

<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#1-ìì—°ì–´-ì „ì²˜ë¦¬">ìì—°ì–´ ì „ì²˜ë¦¬</a>
      <ul>
        <li><a href="#11-ìì—°ì–´ì²˜ë¦¬ì˜-ë‹¨ê³„">ìì—°ì–´ì²˜ë¦¬ì˜ ë‹¨ê³„</a></li>
        <li><a href="#12-python-string-ê´€ë ¨-í•¨ìˆ˜">Python string ê´€ë ¨ í•¨ìˆ˜</a></li>
        <li><a href="#13-ì½”ë“œ-ì‹¤ìŠµ">ì½”ë“œ ì‹¤ìŠµ</a></li>
      </ul>
    </li>
    <li>
      <a href="#2-ìì—°ì–´-í† í¬ë‚˜ì´ì§•">ìì—°ì–´ í† í¬ë‚˜ì´ì§•</a>
      <ul>
        <li><a href="#21-í•œêµ­ì–´-í† í°í™”">í•œêµ­ì–´ í† í°í™”</a></li>
        <li><a href="#22-ì½”ë“œ-ì‹¤ìŠµ">ì½”ë“œ ì‹¤ìŠµ</a></li>
      </ul>
    </li>
    <li><a href="#further-reading">Further Reading</a></li>
  </ol>
</details>

## 1. ìì—°ì–´ ì „ì²˜ë¦¬

### 1.1 ìì—°ì–´ì²˜ë¦¬ì˜ ë‹¨ê³„

![img](../../../assets/img/p-stage/klue_02_01.PNG)

- Task ì„¤ê³„
- í•„ìš” ë°ì´í„° ìˆ˜ì§‘
- í†µê³„í•™ì  ë¶„ì„
    - Token ê°¯ìˆ˜, dictionary ì •ì˜
- ì „ì²˜ë¦¬
    - ê°œí–‰ë¬¸ì, íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ë“±ë“± ì œê±°
- Tagging
- Tokenizing
    - ì–´ì ˆ, í˜•íƒœì†Œ, WordPiece ë“±
- ëª¨ë¸ ì„¤ê³„
- ëª¨ë¸ êµ¬í˜„
- ì„±ëŠ¥ í‰ê°€

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

### 1.2 Python string ê´€ë ¨ í•¨ìˆ˜
- `upper`
- `lower`
- `capitalize`
- `title`
- `swapcase`
- `strip`
- `rstrip`
- `lstrip`
- `replace`
- `split`
- `join`
- `splitlines`
- `isdigit`
- `isalpha`
- `isalnum`
- `islower`
- `isupper`
- `isspace`
- `startswith`
- `endswith`
- `count`
- `find`
- `rfind`
- `index`
    - ë’¤ì˜ ì¸ìë¡œ ì¤‘ë³µ ì²˜ë¦¬ ê°€ëŠ¥
- `rindex`

ìœ„ì™€ ì¹œìˆ™í•´ì§€ëŠ” ê²ƒ ë„ˆë¬´ë‚˜ ì¤‘ìš”!

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

### 1.3 ì½”ë“œ ì‹¤ìŠµ

```python
# urlì„ ì…ë ¥ìœ¼ë¡œ textë¥¼ ì¶”ì¶œí•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
# https://github.com/codelucas/newspaper
$ pip install newspaper3k

import newspaper
newspaper.languages()
```
```

Your available languages are:

input code		full name
  en			  English
  et			  Estonian
  bg			  Bulgarian
  mk			  Macedonian
  he			  Hebrew
  fi			  Finnish
  ru			  Russian
  pt			  Portuguese
  it			  Italian
  de			  German
  no			  Norwegian
  da			  Danish
  zh			  Chinese
  ja			  Japanese
  es			  Spanish
  id			  Indonesian
  ro			  Romanian
  tr			  Turkish
  pl			  Polish
  be			  Belarusian
  vi			  Vietnamese
  el			  Greek
  ko			  Korean
  sl			  Slovenian
  nl			  Dutch
  hu			  Hungarian
  sv			  Swedish
  uk			  Ukrainian
  hi			  Hindi
  fa			  Persian
  sr			  Serbian
  fr			  French
  hr			  Croatian
  ar			  Arabic
  nb			  Norwegian (BokmÃ¥l)
  sw			  Swahili
```

- ë‰´ìŠ¤ í¬ë¡¤ë§!

```python
from newspaper import Article

article = Article(news_url, language='ko')

article.download()
article.parse()

print('title:', article.title)

print('context:', article.text)
```

- ì „ì²˜ë¦¬ ì‹¤ìŠµì„ ìœ„í•´ ì„ì˜ì˜ í…ìŠ¤íŠ¸ ì¶”ê°€

```python
context = article.text.split('\n')
context.append("<h1>ì—¬ê¸°ì— íƒœê·¸ê°€ ìˆë„¤ìš”!</h1> <h3>ì´ê³³ì—ë„ íƒœê·¸ê°€ ìˆêµ¬ìš”</h3> htmlì€ <b>íƒœê·¸</b>ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ <b>ì§„í•˜ê²Œ</b> ë§Œë“¤ ìˆ˜ë„ ìˆê³ , <u>ë°‘ì¤„</u>ì„ ì¹  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜")
context.append("(ì„œìš¸=ìœ„í‚¤íŠ¸ë¦¬) ê¹€ì„±í˜„ ê¸°ì (seonghkim@smilegate.com) <ì €ì‘ê¶Œì(c) ë¬´ë‹¨ì „ì¬-ì¬ë°°í¬ ê¸ˆì§€> â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜")
context.append("(ì‚¬ì§„=ìœ„í‚¤íŠ¸ë¦¬, ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬ ê¸ˆì§€) â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜")
context.append("#ì´ì„¸ëŒ #ì•ŒíŒŒê³  #ì¸ê³µì§€ëŠ¥ #ë”¥ëŸ¬ë‹ #ë°”ë‘‘")
```
```
0
1 <h1>ì—¬ê¸°ì— íƒœê·¸ê°€ ìˆë„¤ìš”!</h1> <h3>ì´ê³³ì—ë„ íƒœê·¸ê°€ ìˆêµ¬ìš”</h3> htmlì€ <b>íƒœê·¸</b>ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ <b>ì§„í•˜ê²Œ</b> ë§Œë“¤ ìˆ˜ë„ ìˆê³ , <u>ë°‘ì¤„</u>ì„ ì¹  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜
2 (ì„œìš¸=ìœ„í‚¤íŠ¸ë¦¬) ê¹€ì„±í˜„ ê¸°ì (seonghkim@smilegate.com) <ì €ì‘ê¶Œì(c) ë¬´ë‹¨ì „ì¬-ì¬ë°°í¬ ê¸ˆì§€> â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜
3 (ì‚¬ì§„=ìœ„í‚¤íŠ¸ë¦¬, ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬ ê¸ˆì§€) â€˜<br>ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦<br>â€˜
4 #ì´ì„¸ëŒ #ì•ŒíŒŒê³  #ì¸ê³µì§€ëŠ¥ #ë”¥ëŸ¬ë‹ #ë°”ë‘‘
```

- HTML í…Œê·¸ ì œê±°!

```python
import re


def remove_html(texts):
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    ``<p>ì•ˆë…•í•˜ì„¸ìš” ã…ã… </p>`` -> ``ì•ˆë…•í•˜ì„¸ìš” ã…ã… ``
    """
    preprcessed_text = []
    for text in texts:
        text = re.sub(r"<[^>]+>\s+(?=<)|<[^>]+>", "", text).strip()
        if text:
            preprcessed_text.append(text)
    return preprcessed_text
```

- ë¬¸ì¥ ë¶„ë¦¬
    - í˜„ì›…ì´ì˜ kss ì‚¬ìš©
    - ë£° ê¸°ë°˜

```python
# Korean Sentence Splitter
$ pip install kss

import kss

sents = []

for sent in context:
    sent = sent.strip()
    if sent:
        splited_sent = kss.split_sentences(sent)
        sents.extend(splited_sent)
```
```
0 ì—¬ê¸°ì— íƒœê·¸ê°€ ìˆë„¤ìš”!
1 ì´ê³³ì—ë„ íƒœê·¸ê°€ ìˆêµ¬ìš”
2 htmlì€ íƒœê·¸ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œì…ë‹ˆë‹¤.
3 í…ìŠ¤íŠ¸ë¥¼ ì§„í•˜ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆê³ , ë°‘ì¤„ì„ ì¹  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. â€˜ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦â€˜
4 (ì„œìš¸=ìœ„í‚¤íŠ¸ë¦¬) ê¹€ì„±í˜„ ê¸°ì (seonghkim@smilegate.com) â€˜ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦â€˜
5 (ì‚¬ì§„=ìœ„í‚¤íŠ¸ë¦¬, ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬ ê¸ˆì§€) â€˜ì´ ì¤„ì€ ì‹¤ì œ ë‰´ìŠ¤(news,)ì— í¬í•¨ë˜ì§€ ì•Šì€ ì„ì‹œ ë°ì´í„°ì„ì„ ì•Œë¦½ë‹ˆë‹¤â€¦â€˜
6 #ì´ì„¸ëŒ #ì•ŒíŒŒê³  #ì¸ê³µì§€ëŠ¥ #ë”¥ëŸ¬ë‹ #ë°”ë‘‘
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

- Normalizing

```python
# remove email
text = re.sub(r"[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "", text).strip()

# remove hashtag
text = re.sub(r"#\S+", "", text).strip()

# remove user mention
text = re.sub(r"@\w+", "", text).strip()

# remove url
text = re.sub(r"(http|https)?:\/\/\S+\b|www\.(\w+\.)+\S*", "", text).strip()
text = re.sub(r"pic\.(\w+\.)+\S*", "", text).strip()

# remove bad char
bad_chars = {"\u200b": "", "â€¦": " ... ", "\ufeff": ""}
text = text.replace(bad_char, bad_chars[bad_char])
text = re.sub(r"[\+Ã¡?\xc3\xa1]", "", text)

# remove press
re_patterns = [
    r"\([^(]*?(ë‰´ìŠ¤|ê²½ì œ|ì¼ë³´|ë¯¸ë””ì–´|ë°ì¼ë¦¬|í•œê²¨ë¡€|íƒ€ì„ì¦ˆ|ìœ„í‚¤íŠ¸ë¦¬)\)",
    r"[ê°€-í£]{0,4} (ê¸°ì|ì„ ì„ê¸°ì|ìˆ˜ìŠµê¸°ì|íŠ¹íŒŒì›|ê°ì›ê¸°ì|ë…¼ì„¤ê³ ë¬¸|í†µì‹ ì›|ì—°êµ¬ì†Œì¥) ",  # ì´ë¦„ + ê¸°ì
    r"[ê°€-í£]{1,}(ë‰´ìŠ¤|ê²½ì œ|ì¼ë³´|ë¯¸ë””ì–´|ë°ì¼ë¦¬|í•œê²¨ë¡€|íƒ€ì„|ìœ„í‚¤íŠ¸ë¦¬)",  # (... ì—°í•©ë‰´ìŠ¤) ..
    r"\(\s+\)",  # (  )
    r"\(=\s+\)",  # (=  )
    r"\(\s+=\)",  # (  =)
]
text = re.sub(re_pattern, "", text).strip()

# remove copyright
re_patterns = [
    r"\<ì €ì‘ê¶Œì(\(c\)|â“’|Â©|\(Copyright\)|(\(c\))|(\(C\))).+?\>",
    r"ì €ì‘ê¶Œì\(c\)|â“’|Â©|(Copyright)|(\(c\))|(\(C\))"
]
text = re.sub(re_pattern, "", text).strip()

# remove photo info
text = re.sub(r"\(ì¶œì²˜ ?= ?.+\) |\(ì‚¬ì§„ ?= ?.+\) |\(ìë£Œ ?= ?.+\)| \(ìë£Œì‚¬ì§„\) |ì‚¬ì§„=.+ê¸°ì ", "", text).strip()

# remove useless breaket
# ``ìˆ˜í•™(,)`` -> ``ìˆ˜í•™``
# ``ìˆ˜í•™(æ•¸å­¸,) -> ``ìˆ˜í•™(æ•¸å­¸)``
bracket_pattern = re.compile(r"\((.*?)\)")

# repeat normalize
$ pip install soynlp
from soynlp.normalizer import *
print(repeat_normalize('ì™€í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•«', num_repeats=2))
>>> ì™€í•˜í•˜í•«

# clean punctuation
punct_mapping = {
    "â€˜": "'", "â‚¹": "e", "Â´": "'",
    "Â°": "", "â‚¬": "e", "â„¢": "tm",
    "âˆš": " sqrt ", "Ã—": "x",
    "Â²": "2", "â€”": "-", "â€“": "-",
    "â€™": "'", "_": "-", "`": "'",
    'â€œ': '"', 'â€': '"', 'â€œ': '"',
    "Â£": "e", 'âˆ': 'infinity',
    'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha',
    'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-',
    'Î²': 'beta', 'âˆ…': '', 'Â³':
    '3', 'Ï€': 'pi'
}

# remove repeated spacing
text = re.sub(r"\s+", " ", text).strip()

# remove duplicate sentence
from collections import OrderedDict
texts = list(OrderedDict.fromkeys(texts))

# spacing sentence
$ pip install git+https://github.com/haven-jeon/PyKoSpacing.git
from pykospacing import Spacing
spacing = Spacing()
text = spacing(text)

# ë§ˆì¶¤ë»¡ ê²€ì‚¬
$ pip install git+https://github.com/ssut/py-hanspell.git
from hanspell import spell_checker
sent = "ëŒ€ì²´ ì™œ ì•Šë¼ëŠ”ì§€ ì„¤ëª…ì„ í•´ë°”"
spelled_sent = spell_checker.check(sent)
checked_sent = spelled_sent.checked

# morpheme based filtering
from konlpy.tag import Mecab
mecab = Mecab()
morphs = mecab.pos("ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤.", join=False)
NN_TAGS = ["NNG", "NNP", "NNB", "NP"]
V_TAGS = ["VV", "VA", "VX", "VCP", "VCN", "XSN", "XSA", "XSV"]
J_TAGS = ["JKS", "J", "JO", "JK", "JKC", "JKG", "JKB", "JKV", "JKQ", "JX", "JC", "JKI", "JKO", "JKM", "ETM"]

# excluded word filter
for word in excluded_words:
    if word in text:
        include_flag = True

# remove stopwords
stopwords = ['ì†Œì·¨ìš”', '-', 'ì¡°ë“œìœ…', 'í¬ìŠ¤í„°', 'ì•“ëŠ”', 'ì„œë¦°']

# min max filter
if min_len < len(text) and len(text) < max_len:
```

- ìœ ë‹ˆì½”ë“œ ê¸°ë°˜ filtering

1. ìœ ë‹ˆì½”ë“œë€?
> ìœ ë‹ˆì½”ë“œ(Unicode)ëŠ” ì „ ì„¸ê³„ì˜ ëª¨ë“  ë¬¸ìë¥¼ ì»´í“¨í„°ì—ì„œ ì¼ê´€ë˜ê²Œ í‘œí˜„í•˜ê³  ë‹¤ë£° ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ì‚°ì—… í‘œì¤€ì´ë©°, ìœ ë‹ˆì½”ë“œ í˜‘íšŒ(Unicode Consortium)ê°€ ì œì •í•œë‹¤. ë˜í•œ ì´ í‘œì¤€ì—ëŠ” ISO 10646 ë¬¸ì ì§‘í•©, ë¬¸ì ì¸ì½”ë”©, ë¬¸ì ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤, ë¬¸ìë“¤ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ ë“±ì„ í¬í•¨í•˜ê³  ìˆë‹¤. - ìœ„í‚¤í”¼ë””ì•„ -

* í•œêµ­ì¸ë“¤ì€ ì£¼ë¡œ ì˜ì–´, í•œêµ­ì–´, ìˆ«ìë“¤ì— ìµìˆ™í•´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ê°‘ìê¸° ëŸ¬ì‹œì•„ì–´, ì•„ëì–´, ë¶ˆì–´ì™€ ê°™ì€ ì–¸ì–´ë“¤ì„ ì²˜ë¦¬í•´ì•¼ í•œë‹¤ë©´?!
* ì´ëŸ° ê³ ë¯¼ì„ ìœ ë‹ˆì½”ë“œë¥¼ ì‚¬ìš©í•˜ë©´ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
* ì´ë²ˆ ì¥ì—ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ìœ ë‹ˆì½”ë“œë¥¼ python í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

2. ìœ ë‹ˆì½”ë“œ í‘œí˜„
* ìœ ë‹ˆì½”ë“œëŠ” 16ì§„ìˆ˜ë¡œ í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì˜ˆì œë¥¼ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
* ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë¬¸ìì— ëŒ€í•œ ìœ ë‹ˆì½”ë“œëŠ” https://jrgraphix.net/r/Unicode/0020-007F ì´ ì‚¬ì´íŠ¸ì—ì„œ ì‰½ê²Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
sentence = 'hello world'

print('* ì›ë³¸ ë¬¸ì¥')
print(sentence)

print('\n* 10ì§„ìˆ˜ë¡œ í‘œí˜„ëœ ìœ ë‹ˆì½”ë“œ')
for w in sentence:
  print(ord(w), end=' ') # ë¬¸ì -> 10ì§„ìˆ˜ ë³€í™˜

print('\n\n* 16ì§„ìˆ˜ë¡œ í‘œí˜„ëœ ìœ ë‹ˆì½”ë“œ')
for w in sentence:
  print(hex(ord(w)), end=' ') # ë¬¸ì -> 16ì§„ìˆ˜ ë³€í™˜
```
```
* ì›ë³¸ ë¬¸ì¥
hello world

* 10ì§„ìˆ˜ë¡œ í‘œí˜„ëœ ìœ ë‹ˆì½”ë“œ
104 101 108 108 111 32 119 111 114 108 100

* 16ì§„ìˆ˜ë¡œ í‘œí˜„ëœ ìœ ë‹ˆì½”ë“œ
0x68 0x65 0x6c 0x6c 0x6f 0x20 0x77 0x6f 0x72 0x6c 0x64
```

```python
range_s = int('0370',16) # ê·¸ë¦¬ìŠ¤ ë¬¸ì ìœ ë‹ˆì½”ë“œ ë²”ìœ„
range_e = int('03FF',16) # 16ì§„ìˆ˜ -> 10ì§„ìˆ˜ ë³€í™˜

for i in range(range_s, range_e + 1): #
  print(chr(i), end=' ')
```
```
Í° Í± Í² Í³ Í´ Íµ Í¶ Í· Í¸ Í¹ Íº Í» Í¼ Í½ Í¾ Í¿ Î€ Î Î‚ Îƒ Î„ Î… Î† Î‡ Îˆ Î‰ ÎŠ Î‹ ÎŒ Î Î Î Î Î‘ Î’ Î“ Î” Î• Î– Î— Î˜ Î™ Îš Î› Îœ Î Î ÎŸ Î  Î¡ Î¢ Î£ Î¤ Î¥ Î¦ Î§ Î¨ Î© Îª Î« Î¬ Î­ Î® Î¯ Î° Î± Î² Î³ Î´ Îµ Î¶ Î· Î¸ Î¹ Îº Î» Î¼ Î½ Î¾ Î¿ Ï€ Ï Ï‚ Ïƒ Ï„ Ï… Ï† Ï‡ Ïˆ Ï‰ ÏŠ Ï‹ ÏŒ Ï Ï Ï Ï Ï‘ Ï’ Ï“ Ï” Ï• Ï– Ï— Ï˜ Ï™ Ïš Ï› Ïœ Ï Ï ÏŸ Ï  Ï¡ Ï¢ Ï£ Ï¤ Ï¥ Ï¦ Ï§ Ï¨ Ï© Ïª Ï« Ï¬ Ï­ Ï® Ï¯ Ï° Ï± Ï² Ï³ Ï´ Ïµ Ï¶ Ï· Ï¸ Ï¹ Ïº Ï» Ï¼ Ï½ Ï¾ Ï¿
```

![img](../../../assets/img/p-stage/klue_02_02.PNG)

- ì´ë ‡ê²Œ íŠ¹ì • ì–¸ì–´ë¥¼ ì§€ìš¸ ìˆ˜ ìˆìŒ

```python
def remove_language(range_s, range_e, sentence):
    a = int(range_s, 16) # 16ì§„ìˆ˜ -> 10ì§„ìˆ˜ ë³€í™˜
    b = int(range_e, 16)
    return_sentence = ''
    for i, w in enumerate(sentence):
        if a<= ord(w) and ord(w) <= b:  # ìŒì ˆ ë‹¨ìœ„ë¡œ ì‚¬ì „ì— ì •ì˜í•œ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ë‚´ì— ì¡´ì¬í•˜ëŠ”ê°€
            continue
        return_sentence+=w
    return return_sentence
```

- ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ í•œê¸€ ë¦¬ìŠ¤íŠ¸ë„ ë§Œë“¤ ìˆ˜ ìˆìŒ
    - https://gist.github.com/jinmang2/5319e0d9918e8be0b9586305c53939f9

```python
def nextKorLetterFrom(letter):
    lastLetterInt = 15572643
    if not letter:
        return 'ê°€'
    a = letter
    b = a.encode('utf8')
    c = int(b.hex(), 16)
    if c == lastLetterInt:
        return False
    d = hex(c + 1)
    e = bytearray.fromhex(d[2:])
    flag = True
    while flag:
        try:
            r = e.decode('utf-8')
            flag = False
        except UnicodeDecodeError:
            c = c + 1
            d = hex(c)
            e = bytearray.fromhex(d[2:])
    return e.decode()

def get_all_korean_char():
    flag = True
    k = ""
    koreans = ""
    while flag:
        k = nextKorLetterFrom(k)
        if k is False:
            flag = False
        else:
            koreans += k
    return koreans
```

- ìœ ë‹ˆì½”ë“œ í•¸ë“¤ë§ì‹œ ìœ ì˜í•  ì !
- ë„ì–´ì“°ê¸° ìœ ë‹ˆì½”ë“œê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ!
- ì£¼ì˜ì£¼ì˜

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

## 2. ìì—°ì–´ í† í¬ë‚˜ì´ì§•

### 2.1 í•œêµ­ì–´ í† í°í™”

**í† í°í™”(Tokenizing)**
- ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í† í°(Token)ì´ë¼ ë¶ˆë¦¬ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…
- í† í°ì´ ë˜ëŠ” ê¸°ì¤€ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ(ì–´ì ˆ, ë‹¨ì–´, í˜•íƒœì†Œ, ìŒì ˆ, ìì†Œ ë“±)

**ë¬¸ì¥ í† í°í™”(Sentence Tokenizing)**
- ë¬¸ì¥ ë¶„ë¦¬

**ë‹¨ì–´ í† í°í™”(Word Tokenizing)**
- êµ¬ë‘ì  ë¶„ë¦¬, ë‹¨ì–´ ë¶„ë¦¬

**í•œêµ­ì–´ì—”?**
- í•œêµ­ì–´ëŠ” ì˜ì–´ì™€ ê°™ì€ êµ´ì ˆì–´ì™€ ë‹¤ë¥´ê²Œ êµì°©ì–´ì„
- ë„ì–´ì“°ê¸°ë§Œìœ¼ë¡œëŠ” ì „ì²˜ë¦¬ ë¶€ì¡±
- í˜•íƒœì†Œë„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

### 2.2 ì½”ë“œ ì‹¤ìŠµ

- ë°ì´í„°ë¥¼ colabì—ì„œ ë‹¤ìš´ë¡œë“œ

```python
data = open('my_data/wiki_20190620_small.txt', 'r', encoding='utf-8')
lines = data.readlines() # ì „ì²´ ë¬¸ì¥ì„ listì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
for line in lines[0:5]:
    print(line, end="")
```
```
ì œì„ìŠ¤ ì–¼ "ì§€ë¯¸" ì¹´í„° ì£¼ë‹ˆì–´ëŠ” ë¯¼ì£¼ë‹¹ ì¶œì‹  ë¯¸êµ­ 39ë²ˆì§¸ ëŒ€í†µë ¹ ì´ë‹¤.
ì§€ë¯¸ ì¹´í„°ëŠ” ì¡°ì§€ì•„ì£¼ ì„¬í„° ì¹´ìš´í‹° í”Œë ˆì¸ìŠ¤ ë§ˆì„ì—ì„œ íƒœì–´ë‚¬ë‹¤.
ì¡°ì§€ì•„ ê³µê³¼ëŒ€í•™êµë¥¼ ì¡¸ì—…í•˜ì˜€ë‹¤.
ê·¸ í›„ í•´êµ°ì— ë“¤ì–´ê°€ ì „í•¨Â·ì›ìë ¥Â·ì ìˆ˜í•¨ì˜ ìŠ¹ë¬´ì›ìœ¼ë¡œ ì¼í•˜ì˜€ë‹¤.
1953ë…„ ë¯¸êµ­ í•´êµ° ëŒ€ìœ„ë¡œ ì˜ˆí¸í•˜ì˜€ê³  ì´í›„ ë•…ì½©Â·ë©´í™” ë“±ì„ ê°€ê¿” ë§ì€ ëˆì„ ë²Œì—ˆë‹¤.
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

#### ì–´ì ˆ ë‹¨ìœ„ tokenizing
- ëª¨ë“  ë¬¸ì¥ì„ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë¶„ë¦¬

```python
text = "ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."
tokenized_text = text.split(" ")    # split í•¨ìˆ˜ëŠ” ì…ë ¥ stringì— ëŒ€í•´ì„œ íŠ¹ì • stringì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.
print(tokenized_text)  
>>> ['ì´ìˆœì‹ ì€', 'ì¡°ì„ ', 'ì¤‘ê¸°ì˜', 'ë¬´ì‹ ì´ë‹¤.']
```

- ì™œ tokení™”ë¥¼ í•´ì£¼ëŠ”ê°€?
    - ì˜ë¯¸ë¥¼ ì§€ë‹Œ ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¶„ì ˆ
    - Modelì˜ í•™ìŠµ ì‹œ, ë™ì¼í•œ sizeë¡œ ì…ë ¥

- ì§ì ‘ tokenizer í´ë˜ìŠ¤ ë§Œë“¤ì–´ë³´ê¸°
```python
print(my_tokenizer.tokenize("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "word"))
print(my_tokenizer.batch_tokenize(["ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "ê·¸ëŠ” ì„ì§„ì™œë€ì„ ìŠ¹ë¦¬ë¡œ ì´ëŒì—ˆë‹¤."], "word"))
```
```
['ì´ìˆœì‹ ì€', 'ì¡°ì„ ', 'ì¤‘ê¸°ì˜', 'ë¬´ì‹ ì´ë‹¤.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
[['ì´ìˆœì‹ ì€', 'ì¡°ì„ ', 'ì¤‘ê¸°ì˜', 'ë¬´ì‹ ì´ë‹¤.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['ê·¸ëŠ”', 'ì„ì§„ì™œë€ì„', 'ìŠ¹ë¦¬ë¡œ', 'ì´ëŒì—ˆë‹¤.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

#### í˜•íƒœì†Œ ë‹¨ìœ„ tokenizing
- konlpyë¡œ ì‹¤ìŠµ

```python
...
elif tokenizer_type == "morph":
    tokenized_text = [lemma[0] for lemma in mecab.pos(text)]
...

print(my_tokenizer.tokenize("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "morph"))
print(my_tokenizer.batch_tokenize(["ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "ê·¸ëŠ” ì„ì§„ì™œë€ì„ ìŠ¹ë¦¬ë¡œ ì´ëŒì—ˆë‹¤."], "morph"))
```
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

```

#### ìŒì ˆ ë‹¨ìœ„ tokenizing
- ìì—°ì–´ë¥¼ í•œ ê¸€ìì”© ë¶„ë¦¬

```python
text = "ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."
tokenized_text = list(text)    # split í•¨ìˆ˜ëŠ” ì…ë ¥ stringì— ëŒ€í•´ì„œ íŠ¹ì • stringì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.
print(tokenized_text)
>>> ['ì´', 'ìˆœ', 'ì‹ ', 'ì€', ' ', 'ì¡°', 'ì„ ', ' ', 'ì¤‘', 'ê¸°', 'ì˜', ' ', 'ë¬´', 'ì‹ ', 'ì´', 'ë‹¤', '.']
```

```python
...
elif tokenizer_type == "syllable":
    tokenized_text = list(text)
...
print(my_tokenizer.tokenize("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "syllable"))
print(my_tokenizer.batch_tokenize(["ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "ê·¸ëŠ” ì„ì§„ì™œë€ì„ ìŠ¹ë¦¬ë¡œ ì´ëŒì—ˆë‹¤."], "syllable"))
```
```
['ì´', 'ìˆœ', 'ì‹ ', 'ì€', ' ', 'ì¡°', 'ì„ ', ' ', 'ì¤‘', 'ê¸°', 'ì˜', ' ', 'ë¬´', 'ì‹ ', 'ì´', 'ë‹¤', '.', '[PAD]', '[PAD]', '[PAD]']
[['ì´', 'ìˆœ', 'ì‹ ', 'ì€', ' ', 'ì¡°', 'ì„ ', ' ', 'ì¤‘', 'ê¸°', 'ì˜', ' ', 'ë¬´', 'ì‹ ', 'ì´', 'ë‹¤', '.', '[PAD]', '[PAD]', '[PAD]'], ['ê·¸', 'ëŠ”', ' ', 'ì„', 'ì§„', 'ì™œ', 'ë€', 'ì„', ' ', 'ìŠ¹', 'ë¦¬', 'ë¡œ', ' ', 'ì´', 'ëŒ', 'ì—ˆ', 'ë‹¤', '.', '[PAD]', '[PAD]']]
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

#### ìì†Œ ë‹¨ìœ„ tokenizing
- í•œê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ìë„ ìµœëŒ€ ì´ˆì„±, ì¤‘ì„±, ì¢…ì„± ì´ 3ê°œì˜ ìì†Œë¡œ ë¶„ë¦¬ê°€ ê°€ëŠ¥
- hgtk ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

```python
$ pip install hgtk

import hgtk
text = "ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."
tokenized_text = list(hgtk.text.decompose(text))
print(tokenized_text)
>>> ['ã…‡', 'ã…£', 'á´¥', 'ã……', 'ã…œ', 'ã„´', 'á´¥', 'ã……', 'ã…£', 'ã„´', 'á´¥', 'ã…‡', 'ã…¡', 'ã„´', 'á´¥', ' ', 'ã…ˆ', 'ã…—', 'á´¥', 'ã……', 'ã…“', 'ã„´', 'á´¥', ' ', 'ã…ˆ', 'ã…œ', 'ã…‡', 'á´¥', 'ã„±', 'ã…£', 'á´¥', 'ã…‡', 'ã…¢', 'á´¥', ' ', 'ã…', 'ã…œ', 'á´¥', 'ã……', 'ã…£', 'ã„´', 'á´¥', 'ã…‡', 'ã…£', 'á´¥', 'ã„·', 'ã…', 'á´¥', '.']
```

```python
...
elif tokenizer_type == "jaso":
    tokenized_text = list(hgtk.text.decompose(text))
...

print(my_tokenizer.tokenize("ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "jaso"))
print(my_tokenizer.batch_tokenize(["ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.", "ê·¸ëŠ” ì„ì§„ì™œë€ì„ ìŠ¹ë¦¬ë¡œ ì´ëŒì—ˆë‹¤."], "jaso"))
```
```
['ã…‡', 'ã…£', 'á´¥', 'ã……', 'ã…œ', 'ã„´', 'á´¥', 'ã……', 'ã…£', 'ã„´', 'á´¥', 'ã…‡', 'ã…¡', 'ã„´', 'á´¥', ' ', 'ã…ˆ', 'ã…—', 'á´¥', 'ã……']
[['ã…‡', 'ã…£', 'á´¥', 'ã……', 'ã…œ', 'ã„´', 'á´¥', 'ã……', 'ã…£', 'ã„´', 'á´¥', 'ã…‡', 'ã…¡', 'ã„´', 'á´¥', ' ', 'ã…ˆ', 'ã…—', 'á´¥', 'ã……'], ['ã„±', 'ã…¡', 'á´¥', 'ã„´', 'ã…¡', 'ã„´', 'á´¥', ' ', 'ã…‡', 'ã…£', 'ã…', 'á´¥', 'ã…ˆ', 'ã…£', 'ã„´', 'á´¥', 'ã…‡', 'ã…™', 'á´¥', 'ã„¹']]
```

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

#### WordPiece tokenizing

```python
$ pip install transformers

from tokenizers import BertWordPieceTokenizer

# Initialize an empty tokenizer
wp_tokenizer = BertWordPieceTokenizer(
    clean_text=True,    # [ì´ìˆœì‹ , ##ì€, ' ', ì¡°ì„ ]
    handle_chinese_chars=True,
    strip_accents=False,    # True: [YepHamza] -> [Yep, Hamza]
    lowercase=False,
)

# And then train
wp_tokenizer.train(
    files="my_data/wiki_20190620_small.txt",
    vocab_size=10000,
    min_frequency=2,
    show_progress=True,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
    limit_alphabet=1000,
    wordpieces_prefix="##"
)

# Save the files
wp_tokenizer.save_model("wordPieceTokenizer", "my_tokenizer")
```
```
Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
['ì´', '##ìˆœ', '##ì‹ ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ì´', '##ë‹¤', '.']
[706, 1350, 7569, 2001, 755, 2603, 453, 8419, 1076, 16]
```

|ì œëª©|ì–´ì ˆ|ìŒì ˆ|í˜•íƒœì†Œ|Wordpiece|
|------|---|---|---|---|
|Unique word|51975|1546|16292|9591|
|ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°œìˆ˜ì˜ í‰ê· |12|56|29|23|
|ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°œìˆ˜ì˜ ì¤‘ì•™ê°’|12|50|27|21|
|ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°œìˆ˜ì˜ ìµœëŒ€/ìµœì†Œê°’|77/1|374/0|185/0|179/0|
|ë¹ˆë²ˆí•œ ìƒìœ„ 5ê°œ ë‹¨ì–´|['ìˆë‹¤.', 'ìˆ˜', 'ì´', 'ê·¸', 'í•œë‹¤.']|[' ', 'ì´', 'ë‹¤', 'ì˜', '.']|['í•˜', '.', 'ì´', 'ì˜', 'ëŠ”']|['.', ',', '##ì˜', '##ì—', '##ì„']|

- ë™ì¼í•œ ì½”í¼ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ë”ë¼ë„ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì €ì˜ íŠ¹ì„±ì— ë”°ë¼ ë§ì´ ë‹¬ë¦¬ì§

<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>

## Further Reading
- [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì†Œê°œ)](https://www.youtube.com/watch?v=9QW7QL8fvv0)
- [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì‹¤ìŠµ)](https://www.youtube.com/watch?v=HIcXyyzefYQ)


<br/>
<div align="right">
    <b><a href="#2ê°•-ìì—°ì–´ì˜-ì „ì²˜ë¦¬">â†¥ back to top</a></b>
</div>
<br/>
