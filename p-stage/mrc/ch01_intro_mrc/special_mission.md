# Mission 0
preë¯¸ì…˜ì—ì„œëŠ” Huggingface Transformerë¥¼ ë¹ ë¥´ê²Œ ë¦¬ë§ˆì¸ë“œ í•©ë‹ˆë‹¤.

ì´ë¯¸ NLP, KLUE ê°•ì˜ë¥¼ ê±°ì³ì˜¤ì‹  ìº í¼ë¶„ë“¤ê»˜ TransformersëŠ” ìµìˆ™í•˜ì‹¤í…ë°ìš”

í•˜ì§€ë§Œ, MRCë¥¼ ì‹œì‘í•˜ê¸°ì— ì•ì„œ ë¦¬ë§ˆì¸ë“œ + ë†“ì¹˜ê¸° ì‰¬ìš´ ë””í…Œì¼ì„ ì‚´í´ë³´ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•˜ê³ ì ë³¸ ë¯¸ì…˜ì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.

## Contents List

### 1. Huggingface Transformers ë¹ ë¥´ê²Œ í›‘ì–´ë³´ê¸°

**â›”ï¸ tokenizer ì‚¬ìš©ì‹œ ì£¼ì˜ì‚¬í•­**

1. train dataì˜ ì–¸ì–´ë¥¼ ì´í•´ í•  ìˆ˜ ìˆëŠ” tokenizerì¸ì§€ í™•ì¸
2. ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” pretrained modelê³¼ ë™ì¼í•œ tokenizerì¸ì§€ í™•ì¸

  > ì ì ˆí•œ tokenizerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° **vocab size mismatch**ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜ **special tokenì´ `[unk]`**ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” ğŸ¤¦ğŸ»â€â™€ï¸ëŒ€ì°¸ì‚¬ğŸ¤¦ğŸ»â€â™‚ï¸ê°€ ë²Œì–´ì§ˆ ìˆ˜ ìˆìŒ

3. ë‹¨ì–´ì˜ ê°œìˆ˜ì™€ special tokenì´ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ëª¨ë¸ì€ (ì˜ˆë¥¼ë“¤ì–´ klueì˜ roberta, bert) tokenizerë¥¼ crossë¡œ ì‚¬ìš© **'í•  ìˆ˜ë„'** ìˆì§€ë§Œ ì˜³ì€ ë°©ë²•ì€ ì•„ë‹˜
  * ì²¨ì–¸í•˜ìë©´, ê³µê°œëœ ì˜ì–´ bertì™€ robertaëŠ” tokenizerê°€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (bert vocab 28996ê°œ, roberta vocab 50265ê°œ)
  * klue bertëŠ” ë™ì¼í•œ ê¸°ê´€ì—ì„œ ìƒì„±ëœ ëª¨ë¸ì´ë¯€ë¡œ 32000ê°œë¡œ ì´ vocab ì‚¬ì´ì¦ˆê°€ ë™ì¼í•˜ì§€ë§Œ ì´ëŠ” ìš°ì—°ì˜ ì¼ì¹˜ì…ë‹ˆë‹¤.


**â›”ï¸ config ì‚¬ìš©ì‹œ ì£¼ì˜ì‚¬í•­**

ì–´ë–¤ ê²½ìš°ì—ëŠ” configë¥¼ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ê¸°ë„ í•˜ëŠ”ë°, ë°”ê¾¸ì–´ë„ ë˜ëŠ” configì™€ ë°”ê¾¸ì§€ ë§ì•„ì•¼ í•˜ëŠ” configê°€ ì •í•´ì ¸ ìˆìŠµë‹ˆë‹¤.

**ë°”ê¾¸ë©´ ì•ˆë˜ëŠ” config**
* Pretrained model ì‚¬ìš©ì‹œ hidden dimë“± ì´ë¯¸ ì •í•´ì ¸ ìˆëŠ” ëª¨ë¸ì˜ ì•„í‚¤í…ì³ ì„¸íŒ…ì€ ìˆ˜ì •í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
* ì´ë¥¼ ìˆ˜ì •í•´ë²„ë¦´ ê²½ìš° ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜, ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ í•™ìŠµ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë°”ê¾¸ì–´ë„ ë˜ëŠ” config**
* vocabì˜ ê²½ìš° special tokenì„ ì¶”ê°€í•œë‹¤ë©´ configë¥¼ ì¶”ê°€í•œ vocabì˜ ê°œìˆ˜ë§Œí¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•´ì•¼í•©ë‹ˆë‹¤.
* downstream taskë¥¼ ìœ„í•´ ëª‡ê°€ì§€ configë¥¼ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. (ì•„ë˜ì—ì„œ ì˜ˆì‹œë¥¼ ì‚´í´ë´…ì‹œë‹¤)

**ìœ ìš©í•œ config ì‚¬ìš©ë²•**
- argumentë¡œ ë„˜ê²¨ì¤„ ê²½ìš° ê¸°ì¡´ì— ì„¤ì •ë˜ìˆë˜ keyë§Œ ì¶”ì¶œí•´ì„œ configì— ë„£ìŒ

```python
custom_config = {~}
config = AutoConfig.from_pretrained(model_name)
config.update(config)
```

## Huggingface Trainer

**í•˜ì§€ë§Œ Trainerê°€ 'í•­ìƒ' ì¢‹ì„ê¹Œìš” ?**

- **ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë“ˆì˜ ì½”ë“œëŠ” legacyê°€ ì¡´ì¬í•  ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤.**
    :ë”°ë¼ì„œ ë²„ì „ì´ ë°”ë€” ë•Œ ë§ˆë‹¤ ë³€ë™ë˜ëŠ” ì‚¬í•­ì´ ë§ì•„ì§€ê³  ì½”ë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼í•˜ëŠ” ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
    : pytorch lightningì´ ëŒ€í‘œì ìœ¼ë¡œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê²ªê³  ìˆìœ¼ë©°, transformersë„ ì˜ˆì™¸ëŠ” ì•„ë‹™ë‹ˆë‹¤.
    : ë”°ë¼ì„œ TrainerëŠ” ëª¨ë“  ìƒí™©ì—ì„œ ì •ë‹µì´ ë  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤

- **ìµœëŒ€í•œ í¸ë¦¬í•¨ì„ ì´ìš©í•˜ë˜, ë™ì‘ ì›ë¦¬ë¥¼ ì‚´í´ë³´ëŠ” ê³¼ì •ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.**

- Trainerì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê³ , ë‚´ê°€ í•™ìŠµí•  ëª¨ë¸ì„ ìœ„í•œ Trainerë¥¼ ë§Œë“¤ì–´ë³´ëŠ”ê²ƒë„ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤
    - Trainerì— ì›í•˜ëŠ” í•¨ìˆ˜ ì˜¤ë²„ë¼ì´ë”© í•˜ì—¬ ìˆ˜ì •í•˜ê¸° (general taskì— ì í•©)
    - Custome Trainer ë§Œë“¤ì–´ë³´ê¸° (general taskê°€ ì•„ë‹Œê²½ìš° ìœ ìš©í•¨)

## Advanced tutorial

### token ì¶”ê°€í•˜ê¸°

```python
# special token ì¶”ê°€í•˜ê¸°
# ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€? special argumentê°€ Trueì¸ ìƒíƒœë¡œ ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ê²Œ ëœë‹¤
special_tokens_dict = {'additional_special_tokens': ['[special1]','[special2]','[special3]','[special4]']}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

# token ì¶”ê°€í•˜ê¸°
# ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€? special argumentê°€ Falseì¸ ìƒíƒœë¡œ ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ê²Œ ëœë‹¤
new_tokens = ['COVID', 'hospitalization']
num_added_toks = tokenizer.add_tokens(new_tokens)

# tokenizer config ìˆ˜ì •í•´ì£¼ê¸° (ì¶”í›„ì— ë°œìƒí•  ì—ëŸ¬ë¥¼ ì¤„ì´ê¸° ìœ„í•´)
config.vocab_size = len(tokenizer)

# modelì˜ token embedding ì‚¬ì´ì¦ˆ ìˆ˜ì •í•˜ê¸°
model.resize_token_embeddings(len(tokenizer))
```

Q : special tokenì„ ì¶”ê°€í•  ë•Œ í•­ìƒ resizeë¥¼ í•´ì£¼ì–´ì•¼ í•˜ë‚˜ìš” ?

A : ê¼­ ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì€ resizeë¥¼ í•˜ì§€ì•Šê³ ë„ ëª¨ë¸ì— ìƒˆë¡œìš´ vocabì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ì—¬ë¶„ì˜ vocab ìë¦¬ë¥¼ ë§Œë“¤ì–´ ë‘ì—ˆìŠµë‹ˆë‹¤. ì—¬ë¶„ì˜ vocab ê°œìˆ˜ëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ë‹ˆ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

- ëª¨ë“  ëª¨ë¸ì´ dummy vocabì„ ê³ ë ¤í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ê³  í•¨
- SKT/KoBERTëŠ” dummy vocabì´ ì—†ê³  ì¶”ê°€ vocabì„ ë„£ì„ ê²½ìš°ì— gluonnlpë¥¼ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•œë‹¤ê³  í•¨

## [CLS] í† í° ì¶”ì¶œí•˜ê¸°
- ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¦„ (ì„¤ê³„ë§ˆë‹¤)
- ì°¸ê³  : [CLS] í† í°ì€ ì •ë§ ë¬¸ì¥ì„ ëŒ€í‘œí• ê¹Œ ?
    - BERTì˜ ì €ì ë˜í•œ [CLS]ê°€ Sentence representationì´ë€ ê²ƒì„ ë³´ì¥í•  ìˆ˜ ì—†ë‹¤ê³  ë°í˜
    - https://github.com/google-research/bert/issues/164
    - ìš°ë¦¬ê°€ íŠ¹ì • taskë¥¼ ìˆ˜í–‰í•  ë•Œ [CLS] í† í°ì´ ë‹¹ì—°íˆ ë¬¸ì¥ì„ ëŒ€í‘œí•´ì¤„ ê²ƒì´ë¼ëŠ” ê°€ì •ì„ ê°€ì§€ëŠ” ê²ƒì€ ìœ„í—˜í•¨
    - ì‹¤í—˜ì„ í†µí•´ ì–´ë–¤ í† í°ì´ ì¤‘ìš”í•œ ì§€ ì°¾ì•„ë³¼ ê²ƒ!
    - ì½ì–´ë³¼ë§Œí•œ ë…¼ë¬¸ ì¶”ì²œ (SBERT)
        - ë…¼ë¬¸ : https://arxiv.org/pdf/1908.10084.pdf
        - **The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014).**
        - ìš”ì•½) avgë‚˜ CLSë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¼ë°˜ì ì´ì§€ë§Œ ì´ê±´ GLoVeì˜ avgë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ìŒ
- ğŸšªâœŠKnock Knockì„ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ì•Œë¦¼ë°›ê¸°

> pip install knockknock

```
from knockknock import slack_sender

webhook_url = "<webhook_url_to_your_slack_room>"
@slack_sender(webhook_url=webhook_url, channel="<your_favorite_slack_channel>")
def train_your_nicest_model(your_nicest_parameters):
    import time
    time.sleep(10000)
    return {'loss': 0.9} # Optional return value

```


command lineì—ì„œ ì‹¤í–‰í•˜ê¸°
```
knockknock slack \
    --webhook-url <webhook_url_to_your_slack_room> \
    --channel <your_favorite_slack_channel> \
    sleep 10

```
